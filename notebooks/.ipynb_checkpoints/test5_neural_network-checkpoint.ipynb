{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Case</th>\n",
       "      <th>Sex</th>\n",
       "      <th>C4A.C4B.4481.34.2</th>\n",
       "      <th>IL10.2773.50.2</th>\n",
       "      <th>MMP9.2579.17.5</th>\n",
       "      <th>CSF3.8952.65.3</th>\n",
       "      <th>GC.6581.50.3</th>\n",
       "      <th>APOB.2797.56.2</th>\n",
       "      <th>CFH.4159.130.1</th>\n",
       "      <th>...</th>\n",
       "      <th>C3.2755.8.2</th>\n",
       "      <th>PPY.4588.1.2</th>\n",
       "      <th>IGFBP2.2570.72.5</th>\n",
       "      <th>APOE.2937.10.2</th>\n",
       "      <th>FGA.FGB.FGG.4907.56.1</th>\n",
       "      <th>PLG.3710.49.2</th>\n",
       "      <th>TNF.5936.53.3</th>\n",
       "      <th>ANGPT2.13660.76.3</th>\n",
       "      <th>CRP.4337.49.2</th>\n",
       "      <th>VCAM1.2967.8.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.069435</td>\n",
       "      <td>-0.034924</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.351996</td>\n",
       "      <td>0.314848</td>\n",
       "      <td>-0.318171</td>\n",
       "      <td>0.195647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057947</td>\n",
       "      <td>0.749855</td>\n",
       "      <td>0.146249</td>\n",
       "      <td>-0.133883</td>\n",
       "      <td>-2.490502</td>\n",
       "      <td>-0.324963</td>\n",
       "      <td>-0.978661</td>\n",
       "      <td>0.039893</td>\n",
       "      <td>-0.266216</td>\n",
       "      <td>-0.038887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.071661</td>\n",
       "      <td>-0.421213</td>\n",
       "      <td>-0.872182</td>\n",
       "      <td>0.101832</td>\n",
       "      <td>-0.296282</td>\n",
       "      <td>0.689710</td>\n",
       "      <td>-0.409555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903393</td>\n",
       "      <td>0.306309</td>\n",
       "      <td>-1.152134</td>\n",
       "      <td>0.421410</td>\n",
       "      <td>0.405951</td>\n",
       "      <td>0.950770</td>\n",
       "      <td>1.549364</td>\n",
       "      <td>1.410669</td>\n",
       "      <td>-1.659654</td>\n",
       "      <td>2.214628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321382</td>\n",
       "      <td>-2.000421</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.952631</td>\n",
       "      <td>-2.252077</td>\n",
       "      <td>-1.264962</td>\n",
       "      <td>...</td>\n",
       "      <td>1.754089</td>\n",
       "      <td>1.248028</td>\n",
       "      <td>1.187176</td>\n",
       "      <td>-1.003851</td>\n",
       "      <td>-0.848056</td>\n",
       "      <td>-0.324963</td>\n",
       "      <td>-1.076009</td>\n",
       "      <td>0.381713</td>\n",
       "      <td>-0.420168</td>\n",
       "      <td>-0.413070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.575056</td>\n",
       "      <td>-0.256073</td>\n",
       "      <td>0.618608</td>\n",
       "      <td>-1.863235</td>\n",
       "      <td>-0.379302</td>\n",
       "      <td>0.689710</td>\n",
       "      <td>-0.636036</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.259195</td>\n",
       "      <td>-0.329593</td>\n",
       "      <td>-0.662532</td>\n",
       "      <td>-0.133883</td>\n",
       "      <td>1.754016</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-1.708552</td>\n",
       "      <td>-1.134225</td>\n",
       "      <td>-1.053209</td>\n",
       "      <td>-0.110263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.512781</td>\n",
       "      <td>0.417969</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.091216</td>\n",
       "      <td>-0.337681</td>\n",
       "      <td>-0.318171</td>\n",
       "      <td>-0.115083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554612</td>\n",
       "      <td>-0.582945</td>\n",
       "      <td>0.512406</td>\n",
       "      <td>0.843965</td>\n",
       "      <td>0.379672</td>\n",
       "      <td>-1.239034</td>\n",
       "      <td>0.858960</td>\n",
       "      <td>-0.683777</td>\n",
       "      <td>-0.344878</td>\n",
       "      <td>-0.487496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group  Case  Sex  C4A.C4B.4481.34.2  IL10.2773.50.2  MMP9.2579.17.5  \\\n",
       "0     A     2    2          -0.069435       -0.034924       -0.126787   \n",
       "1     A     2    2          -1.071661       -0.421213       -0.872182   \n",
       "2     A     2    1           0.321382       -2.000421       -0.126787   \n",
       "3     A     2    2           0.575056       -0.256073        0.618608   \n",
       "4     A     2    2           1.512781        0.417969       -0.126787   \n",
       "\n",
       "   CSF3.8952.65.3  GC.6581.50.3  APOB.2797.56.2  CFH.4159.130.1  ...  \\\n",
       "0        0.351996      0.314848       -0.318171        0.195647  ...   \n",
       "1        0.101832     -0.296282        0.689710       -0.409555  ...   \n",
       "2        0.207792      0.952631       -2.252077       -1.264962  ...   \n",
       "3       -1.863235     -0.379302        0.689710       -0.636036  ...   \n",
       "4        0.091216     -0.337681       -0.318171       -0.115083  ...   \n",
       "\n",
       "   C3.2755.8.2  PPY.4588.1.2  IGFBP2.2570.72.5  APOE.2937.10.2  \\\n",
       "0     0.057947      0.749855          0.146249       -0.133883   \n",
       "1    -0.903393      0.306309         -1.152134        0.421410   \n",
       "2     1.754089      1.248028          1.187176       -1.003851   \n",
       "3    -1.259195     -0.329593         -0.662532       -0.133883   \n",
       "4     0.554612     -0.582945          0.512406        0.843965   \n",
       "\n",
       "   FGA.FGB.FGG.4907.56.1  PLG.3710.49.2  TNF.5936.53.3  ANGPT2.13660.76.3  \\\n",
       "0              -2.490502      -0.324963      -0.978661           0.039893   \n",
       "1               0.405951       0.950770       1.549364           1.410669   \n",
       "2              -0.848056      -0.324963      -1.076009           0.381713   \n",
       "3               1.754016       0.614542      -1.708552          -1.134225   \n",
       "4               0.379672      -1.239034       0.858960          -0.683777   \n",
       "\n",
       "   CRP.4337.49.2  VCAM1.2967.8.1  \n",
       "0      -0.266216       -0.038887  \n",
       "1      -1.659654        2.214628  \n",
       "2      -0.420168       -0.413070  \n",
       "3      -1.053209       -0.110263  \n",
       "4      -0.344878       -0.487496  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data processed on the HPC\n",
    "\n",
    "prs = pd.read_csv(\"../data/protein_prs_cases.csv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "prs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Case          Sex\n",
      "count  6244.000000  6244.000000\n",
      "mean      0.686579     0.598334\n",
      "std       0.463921     0.490274\n",
      "min       0.000000     0.000000\n",
      "25%       0.000000     0.000000\n",
      "50%       1.000000     1.000000\n",
      "75%       1.000000     1.000000\n",
      "max       1.000000     1.000000\n",
      "Group\n",
      "A    3277\n",
      "B     639\n",
      "C     371\n",
      "Name: Case, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Update case and sex from 2/1 and to dummy variables 1/0\n",
    "#Case -> 1 = \"AD\", 0 = \"CTL\"\n",
    "#Sex -> 1 = \"Female\", 0 = \"Male\"\n",
    "\n",
    "if (prs[\"Case\"].max() == 2) | (prs[\"Sex\"].max() == 2):\n",
    "    prs.loc[prs[\"Case\"] == 1, \"Case\"] = 0\n",
    "    prs.loc[prs[\"Case\"] == 2, \"Case\"] = 1\n",
    "\n",
    "    prs.loc[prs[\"Sex\"] == 1, \"Sex\"] = 0\n",
    "    prs.loc[prs[\"Sex\"] == 2, \"Sex\"] = 1\n",
    "else:\n",
    "    print(\"Already updated\")\n",
    "\n",
    "print(prs[[\"Case\", \"Sex\"]].describe())\n",
    "\n",
    "print(prs.groupby(\"Group\")[\"Case\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOAL - predict AD status from 31 proteins using a simple feed forward neural network\n",
    "\n",
    "#Questions\n",
    "#-> how represent the input proteins e.g. float vector of 31 values? What dimensions?\n",
    "#-> how represent the output of AD disease status e.g. a 2 node output layer connected by a softmax / sigmoid function? \n",
    "#-> how structure hidden layers? How many?\n",
    "#-> how initialise layers?\n",
    "#-> what objective, loss function?\n",
    "#-> how setup batch sizes for training? e.g. all samples in one batch, randomised sub-samples, 1 sample at a time?\n",
    "    #-> are batches all calculated at once? e.g. [10, 31] input if batch of 10\n",
    "#-> when do you need to explicitly set a model to train? (varied across examples, not in RNN, is in CNN images)\n",
    "#-> does calculating the accuracy and other metrics during training impact gradient calcs if don't set no_grad?\n",
    "#-> what is best practice for measuring accuracy on test data? Average across batches?\n",
    "\n",
    "#Resources\n",
    "#-> binary classifier gist - https://gist.github.com/santi-pdp/d0e9002afe74db04aa5bbff6d076e8fe\n",
    "#-> binary cross entropy loss - https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a \n",
    "#-> practical stack overflow example - https://stackoverflow.com/questions/62413462/why-is-a-simple-binary-classification-failing-in-a-feedforward-neural-network\n",
    "#-> microsoft binary classifier example (+ code) - https://visualstudiomagazine.com/articles/2020/10/14/pytorch-define-network.aspx\n",
    "#-> more detailed analytics vidya example - https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/\n",
    "#-> MNIST examples - https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/ \n",
    "    #->+ https://www.kdnuggets.com/2018/02/simple-starter-guide-build-neural-network.html\n",
    "    #->+ https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py\n",
    "    #->+ Highly detailed and some useful code snippets - https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "#-> Titanic example (using Softmax) - https://www.kaggle.com/tauseef6462/simple-feedforward-neural-network-using-pytorch \n",
    "#-> other shorter examples - https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb\n",
    "    #->+ https://medium.com/@niranjankumarc/building-a-feedforward-neural-network-using-pytorch-nn-module-52b1d7ea5c3e\n",
    "#-> dataloaders - https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047 \n",
    "\n",
    "\n",
    "#Findings\n",
    "    #->Mean AUC is 0.54 on test data, 0.57 on training, without tuning hyperparameters - comparable to random forest\n",
    "    #->Class imbalance with significantly more cases than controls (~70:30) requires correction to improve prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data into train and test splits and convert to numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(prs.iloc[:, 3:len(prs)].to_numpy(), prs[\"Case\"].to_numpy(), test_size=0.25, random_state=0)\n",
    "\n",
    "# print(x_train.shape, x_train[0:5])\n",
    "# print(y_train.shape, y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to torch tensors\n",
    "x_train, y_train, x_test, y_test = map(torch.tensor, (x_train, y_train, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4683, 31]) tensor([[ 1.7982e+00,  1.3856e+00, -1.2679e-01, -2.6436e-01, -6.6459e-01,\n",
      "         -3.1817e-01,  2.9765e+00, -9.4479e-01,  2.0746e+00, -1.5596e-02,\n",
      "          1.9561e+00, -7.3328e-01, -2.5517e-01, -2.4023e-01,  2.3657e+00,\n",
      "         -2.5215e+00,  1.2740e-01, -3.1030e-02,  1.2908e-01,  2.1609e-01,\n",
      "         -4.6585e-01, -9.5184e-01,  8.3606e-01,  5.8231e-01,  4.2141e-01,\n",
      "         -1.0343e+00, -2.0849e+00,  2.0934e+00, -7.4518e-01,  9.4330e-01,\n",
      "         -4.1696e-01],\n",
      "        [-1.1928e-01, -8.6688e-01, -1.2679e-01, -4.3962e-01,  6.8738e-02,\n",
      "         -1.2442e+00,  1.5526e+00, -3.1274e-03, -4.7507e-01, -1.2216e+00,\n",
      "         -1.2772e-02,  1.4791e+00,  1.0703e-01, -3.3102e-01,  5.1689e-01,\n",
      "          1.8115e-01, -8.5239e-01,  5.8039e-01,  5.9135e-01,  6.5476e-01,\n",
      "          1.5217e+00,  1.2155e+00,  9.0697e-01,  8.1719e-01, -3.3847e+00,\n",
      "          6.5582e-01, -1.7157e+00, -1.9855e+00, -3.6088e-02, -1.0532e+00,\n",
      "          1.5110e-01]], dtype=torch.float64)\n",
      "torch.Size([4683]) tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.size(), x_train[0:2])\n",
    "print(y_train.size(), y_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup torch datasets to load into dataloaders\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "training_data = CustomDataset(x_train, y_train)\n",
    "test_data = CustomDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "#setup dataloaders to handle batches\n",
    "\n",
    "batch_size = 150\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(x_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is this the best way to classify outputs to 1 or 0\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 1-hidden layer neural net (31 -> 50 -> 1) with a SGD optimiser and a BCE Loss objective function\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #initializing layers as per Microsoft article (but does not seem to be a requirement)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(31, 50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.out = nn.Linear(50, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight) \n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight) \n",
    "        torch.nn.init.zeros_(self.out.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        a1 = self.fc1(input)\n",
    "        h1 = self.relu1(a1)\n",
    "        a2 = self.out(h1)\n",
    "        y = self.out_act(a2)\n",
    "        return y\n",
    "    \n",
    "net = Net()\n",
    "opt = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "def confusion_metrics(y_pred, y_true):\n",
    "    auc = roc_auc_score(y_true.detach().numpy(), y_pred.view(-1).detach().numpy().round())\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true.detach().numpy(), y_pred.view(-1).detach().numpy().round()).ravel()\n",
    "    sensitivity = tp / (tp + fn) #-> also recall\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    return auc, sensitivity, specificity, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.728 | Accuracy: 0.529 | AUC: 0.522 | Sensitivity: 0.793 | Specificity: 0.25 | Precision: 0.885\n",
      "Epoch: 2 | Loss: 0.682 | Accuracy: 0.598 | AUC: 0.606 | Sensitivity: 0.75 | Specificity: 0.462 | Precision: 0.682\n",
      "Epoch: 3 | Loss: 0.664 | Accuracy: 0.628 | AUC: 0.405 | Sensitivity: 0.81 | Specificity: 0.0 | Precision: 0.586\n",
      "Epoch: 4 | Loss: 0.655 | Accuracy: 0.642 | AUC: 0.446 | Sensitivity: 0.81 | Specificity: 0.083 | Precision: 0.607\n",
      "Epoch: 5 | Loss: 0.643 | Accuracy: 0.648 | AUC: 0.455 | Sensitivity: 0.818 | Specificity: 0.091 | Precision: 0.643\n",
      "Epoch: 6 | Loss: 0.64 | Accuracy: 0.66 | AUC: 0.455 | Sensitivity: 0.909 | Specificity: 0.0 | Precision: 0.645\n",
      "Epoch: 7 | Loss: 0.634 | Accuracy: 0.666 | AUC: 0.565 | Sensitivity: 0.9 | Specificity: 0.231 | Precision: 0.643\n",
      "Epoch: 8 | Loss: 0.628 | Accuracy: 0.675 | AUC: 0.481 | Sensitivity: 0.963 | Specificity: 0.0 | Precision: 0.812\n",
      "Epoch: 9 | Loss: 0.627 | Accuracy: 0.673 | AUC: 0.523 | Sensitivity: 0.955 | Specificity: 0.091 | Precision: 0.677\n",
      "Epoch: 10 | Loss: 0.626 | Accuracy: 0.673 | AUC: 0.513 | Sensitivity: 0.95 | Specificity: 0.077 | Precision: 0.613\n",
      "Epoch: 11 | Loss: 0.62 | Accuracy: 0.676 | AUC: 0.528 | Sensitivity: 0.957 | Specificity: 0.1 | Precision: 0.71\n",
      "Epoch: 12 | Loss: 0.619 | Accuracy: 0.678 | AUC: 0.583 | Sensitivity: 1.0 | Specificity: 0.167 | Precision: 0.677\n",
      "Epoch: 13 | Loss: 0.615 | Accuracy: 0.682 | AUC: 0.426 | Sensitivity: 0.852 | Specificity: 0.0 | Precision: 0.793\n",
      "Epoch: 14 | Loss: 0.616 | Accuracy: 0.684 | AUC: 0.478 | Sensitivity: 0.957 | Specificity: 0.0 | Precision: 0.688\n",
      "Epoch: 15 | Loss: 0.612 | Accuracy: 0.684 | AUC: 0.479 | Sensitivity: 0.958 | Specificity: 0.0 | Precision: 0.719\n",
      "Epoch: 16 | Loss: 0.613 | Accuracy: 0.682 | AUC: 0.488 | Sensitivity: 0.9 | Specificity: 0.077 | Precision: 0.6\n",
      "Epoch: 17 | Loss: 0.608 | Accuracy: 0.685 | AUC: 0.614 | Sensitivity: 0.955 | Specificity: 0.273 | Precision: 0.724\n",
      "Epoch: 18 | Loss: 0.61 | Accuracy: 0.683 | AUC: 0.5 | Sensitivity: 0.909 | Specificity: 0.091 | Precision: 0.667\n",
      "Epoch: 19 | Loss: 0.609 | Accuracy: 0.681 | AUC: 0.452 | Sensitivity: 0.905 | Specificity: 0.0 | Precision: 0.613\n",
      "Epoch: 20 | Loss: 0.604 | Accuracy: 0.684 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 21 | Loss: 0.606 | Accuracy: 0.68 | AUC: 0.413 | Sensitivity: 0.826 | Specificity: 0.0 | Precision: 0.655\n",
      "Epoch: 22 | Loss: 0.601 | Accuracy: 0.686 | AUC: 0.481 | Sensitivity: 0.962 | Specificity: 0.0 | Precision: 0.781\n",
      "Epoch: 23 | Loss: 0.601 | Accuracy: 0.685 | AUC: 0.514 | Sensitivity: 0.917 | Specificity: 0.111 | Precision: 0.733\n",
      "Epoch: 24 | Loss: 0.602 | Accuracy: 0.685 | AUC: 0.482 | Sensitivity: 0.84 | Specificity: 0.125 | Precision: 0.75\n",
      "Epoch: 25 | Loss: 0.603 | Accuracy: 0.685 | AUC: 0.518 | Sensitivity: 0.952 | Specificity: 0.083 | Precision: 0.645\n",
      "Epoch: 26 | Loss: 0.6 | Accuracy: 0.687 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 27 | Loss: 0.598 | Accuracy: 0.688 | AUC: 0.479 | Sensitivity: 0.958 | Specificity: 0.0 | Precision: 0.719\n",
      "Epoch: 28 | Loss: 0.595 | Accuracy: 0.691 | AUC: 0.582 | Sensitivity: 0.964 | Specificity: 0.2 | Precision: 0.871\n",
      "Epoch: 29 | Loss: 0.601 | Accuracy: 0.684 | AUC: 0.531 | Sensitivity: 1.0 | Specificity: 0.062 | Precision: 0.531\n",
      "Epoch: 30 | Loss: 0.595 | Accuracy: 0.687 | AUC: 0.578 | Sensitivity: 0.957 | Specificity: 0.2 | Precision: 0.733\n",
      "Epoch: 31 | Loss: 0.599 | Accuracy: 0.685 | AUC: 0.509 | Sensitivity: 0.947 | Specificity: 0.071 | Precision: 0.581\n",
      "Epoch: 32 | Loss: 0.594 | Accuracy: 0.689 | AUC: 0.605 | Sensitivity: 0.96 | Specificity: 0.25 | Precision: 0.8\n",
      "Epoch: 33 | Loss: 0.594 | Accuracy: 0.688 | AUC: 0.535 | Sensitivity: 0.958 | Specificity: 0.111 | Precision: 0.742\n",
      "Epoch: 34 | Loss: 0.595 | Accuracy: 0.685 | AUC: 0.509 | Sensitivity: 0.947 | Specificity: 0.071 | Precision: 0.581\n",
      "Epoch: 35 | Loss: 0.595 | Accuracy: 0.688 | AUC: 0.479 | Sensitivity: 0.958 | Specificity: 0.0 | Precision: 0.719\n",
      "Epoch: 36 | Loss: 0.59 | Accuracy: 0.693 | AUC: 0.633 | Sensitivity: 0.933 | Specificity: 0.333 | Precision: 0.933\n",
      "Epoch: 37 | Loss: 0.593 | Accuracy: 0.688 | AUC: 0.44 | Sensitivity: 0.88 | Specificity: 0.0 | Precision: 0.733\n",
      "Epoch: 38 | Loss: 0.594 | Accuracy: 0.687 | AUC: 0.513 | Sensitivity: 0.826 | Specificity: 0.2 | Precision: 0.704\n",
      "Epoch: 39 | Loss: 0.588 | Accuracy: 0.692 | AUC: 0.624 | Sensitivity: 0.962 | Specificity: 0.286 | Precision: 0.833\n",
      "Epoch: 40 | Loss: 0.592 | Accuracy: 0.689 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 41 | Loss: 0.589 | Accuracy: 0.692 | AUC: 0.605 | Sensitivity: 0.96 | Specificity: 0.25 | Precision: 0.8\n",
      "Epoch: 42 | Loss: 0.591 | Accuracy: 0.689 | AUC: 0.472 | Sensitivity: 0.833 | Specificity: 0.111 | Precision: 0.714\n",
      "Epoch: 43 | Loss: 0.589 | Accuracy: 0.69 | AUC: 0.535 | Sensitivity: 0.87 | Specificity: 0.2 | Precision: 0.714\n",
      "Epoch: 44 | Loss: 0.59 | Accuracy: 0.691 | AUC: 0.578 | Sensitivity: 0.957 | Specificity: 0.2 | Precision: 0.733\n",
      "Epoch: 45 | Loss: 0.588 | Accuracy: 0.693 | AUC: 0.59 | Sensitivity: 0.958 | Specificity: 0.222 | Precision: 0.767\n",
      "Epoch: 46 | Loss: 0.587 | Accuracy: 0.691 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 47 | Loss: 0.587 | Accuracy: 0.695 | AUC: 0.605 | Sensitivity: 0.96 | Specificity: 0.25 | Precision: 0.8\n",
      "Epoch: 48 | Loss: 0.587 | Accuracy: 0.693 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 49 | Loss: 0.588 | Accuracy: 0.694 | AUC: 0.514 | Sensitivity: 0.917 | Specificity: 0.111 | Precision: 0.733\n",
      "Epoch: 50 | Loss: 0.585 | Accuracy: 0.695 | AUC: 0.659 | Sensitivity: 0.955 | Specificity: 0.364 | Precision: 0.75\n",
      "Epoch: 51 | Loss: 0.586 | Accuracy: 0.696 | AUC: 0.556 | Sensitivity: 1.0 | Specificity: 0.111 | Precision: 0.75\n",
      "Epoch: 52 | Loss: 0.586 | Accuracy: 0.695 | AUC: 0.495 | Sensitivity: 0.846 | Specificity: 0.143 | Precision: 0.786\n",
      "Epoch: 53 | Loss: 0.588 | Accuracy: 0.693 | AUC: 0.472 | Sensitivity: 0.833 | Specificity: 0.111 | Precision: 0.714\n",
      "Epoch: 54 | Loss: 0.59 | Accuracy: 0.691 | AUC: 0.531 | Sensitivity: 1.0 | Specificity: 0.062 | Precision: 0.531\n",
      "Epoch: 55 | Loss: 0.586 | Accuracy: 0.693 | AUC: 0.542 | Sensitivity: 1.0 | Specificity: 0.083 | Precision: 0.656\n",
      "Epoch: 56 | Loss: 0.584 | Accuracy: 0.697 | AUC: 0.585 | Sensitivity: 0.92 | Specificity: 0.25 | Precision: 0.793\n",
      "Epoch: 57 | Loss: 0.586 | Accuracy: 0.694 | AUC: 0.577 | Sensitivity: 1.0 | Specificity: 0.154 | Precision: 0.645\n",
      "Epoch: 58 | Loss: 0.584 | Accuracy: 0.696 | AUC: 0.562 | Sensitivity: 1.0 | Specificity: 0.125 | Precision: 0.781\n",
      "Epoch: 59 | Loss: 0.588 | Accuracy: 0.69 | AUC: 0.533 | Sensitivity: 0.941 | Specificity: 0.125 | Precision: 0.533\n",
      "Epoch: 60 | Loss: 0.587 | Accuracy: 0.692 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 61 | Loss: 0.583 | Accuracy: 0.694 | AUC: 0.48 | Sensitivity: 0.96 | Specificity: 0.0 | Precision: 0.75\n",
      "Epoch: 62 | Loss: 0.584 | Accuracy: 0.695 | AUC: 0.685 | Sensitivity: 0.952 | Specificity: 0.417 | Precision: 0.741\n",
      "Epoch: 63 | Loss: 0.582 | Accuracy: 0.694 | AUC: 0.48 | Sensitivity: 0.96 | Specificity: 0.0 | Precision: 0.75\n",
      "Epoch: 64 | Loss: 0.585 | Accuracy: 0.692 | AUC: 0.494 | Sensitivity: 0.905 | Specificity: 0.083 | Precision: 0.633\n",
      "Epoch: 65 | Loss: 0.587 | Accuracy: 0.69 | AUC: 0.47 | Sensitivity: 0.857 | Specificity: 0.083 | Precision: 0.621\n",
      "Epoch: 66 | Loss: 0.584 | Accuracy: 0.694 | AUC: 0.615 | Sensitivity: 1.0 | Specificity: 0.231 | Precision: 0.667\n",
      "Epoch: 67 | Loss: 0.579 | Accuracy: 0.698 | AUC: 0.573 | Sensitivity: 0.897 | Specificity: 0.25 | Precision: 0.897\n",
      "Epoch: 68 | Loss: 0.579 | Accuracy: 0.697 | AUC: 0.565 | Sensitivity: 0.963 | Specificity: 0.167 | Precision: 0.839\n",
      "Epoch: 69 | Loss: 0.584 | Accuracy: 0.692 | AUC: 0.494 | Sensitivity: 0.905 | Specificity: 0.083 | Precision: 0.633\n",
      "Epoch: 70 | Loss: 0.578 | Accuracy: 0.697 | AUC: 0.5 | Sensitivity: 1.0 | Specificity: 0.0 | Precision: 0.818\n",
      "Epoch: 71 | Loss: 0.582 | Accuracy: 0.693 | AUC: 0.552 | Sensitivity: 0.95 | Specificity: 0.154 | Precision: 0.633\n",
      "Epoch: 72 | Loss: 0.579 | Accuracy: 0.695 | AUC: 0.528 | Sensitivity: 0.957 | Specificity: 0.1 | Precision: 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Loss: 0.584 | Accuracy: 0.689 | AUC: 0.45 | Sensitivity: 0.833 | Specificity: 0.067 | Precision: 0.517\n",
      "Epoch: 74 | Loss: 0.583 | Accuracy: 0.692 | AUC: 0.509 | Sensitivity: 0.947 | Specificity: 0.071 | Precision: 0.581\n",
      "Epoch: 75 | Loss: 0.581 | Accuracy: 0.695 | AUC: 0.479 | Sensitivity: 0.958 | Specificity: 0.0 | Precision: 0.719\n",
      "Epoch: 76 | Loss: 0.581 | Accuracy: 0.694 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 77 | Loss: 0.582 | Accuracy: 0.69 | AUC: 0.452 | Sensitivity: 0.75 | Specificity: 0.154 | Precision: 0.577\n",
      "Epoch: 78 | Loss: 0.581 | Accuracy: 0.694 | AUC: 0.523 | Sensitivity: 0.955 | Specificity: 0.091 | Precision: 0.677\n",
      "Epoch: 79 | Loss: 0.579 | Accuracy: 0.697 | AUC: 0.65 | Sensitivity: 1.0 | Specificity: 0.3 | Precision: 0.767\n",
      "Epoch: 80 | Loss: 0.577 | Accuracy: 0.697 | AUC: 0.611 | Sensitivity: 1.0 | Specificity: 0.222 | Precision: 0.774\n",
      "Epoch: 81 | Loss: 0.579 | Accuracy: 0.694 | AUC: 0.557 | Sensitivity: 0.913 | Specificity: 0.2 | Precision: 0.724\n",
      "Epoch: 82 | Loss: 0.579 | Accuracy: 0.696 | AUC: 0.591 | Sensitivity: 0.909 | Specificity: 0.273 | Precision: 0.714\n",
      "Epoch: 83 | Loss: 0.577 | Accuracy: 0.698 | AUC: 0.624 | Sensitivity: 0.962 | Specificity: 0.286 | Precision: 0.833\n",
      "Epoch: 84 | Loss: 0.58 | Accuracy: 0.694 | AUC: 0.581 | Sensitivity: 0.947 | Specificity: 0.214 | Precision: 0.621\n",
      "Epoch: 85 | Loss: 0.578 | Accuracy: 0.697 | AUC: 0.463 | Sensitivity: 0.926 | Specificity: 0.0 | Precision: 0.806\n",
      "Epoch: 86 | Loss: 0.579 | Accuracy: 0.694 | AUC: 0.432 | Sensitivity: 0.864 | Specificity: 0.0 | Precision: 0.633\n",
      "Epoch: 87 | Loss: 0.576 | Accuracy: 0.701 | AUC: 0.583 | Sensitivity: 1.0 | Specificity: 0.167 | Precision: 0.844\n",
      "Epoch: 88 | Loss: 0.579 | Accuracy: 0.696 | AUC: 0.5 | Sensitivity: 1.0 | Specificity: 0.0 | Precision: 0.667\n",
      "Epoch: 89 | Loss: 0.576 | Accuracy: 0.697 | AUC: 0.583 | Sensitivity: 1.0 | Specificity: 0.167 | Precision: 0.677\n",
      "Epoch: 90 | Loss: 0.577 | Accuracy: 0.697 | AUC: 0.629 | Sensitivity: 0.95 | Specificity: 0.308 | Precision: 0.679\n",
      "Epoch: 91 | Loss: 0.574 | Accuracy: 0.701 | AUC: 0.583 | Sensitivity: 1.0 | Specificity: 0.167 | Precision: 0.844\n",
      "Epoch: 92 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.448 | Sensitivity: 0.897 | Specificity: 0.0 | Precision: 0.867\n",
      "Epoch: 93 | Loss: 0.578 | Accuracy: 0.696 | AUC: 0.56 | Sensitivity: 0.952 | Specificity: 0.167 | Precision: 0.667\n",
      "Epoch: 94 | Loss: 0.576 | Accuracy: 0.697 | AUC: 0.568 | Sensitivity: 0.955 | Specificity: 0.182 | Precision: 0.7\n",
      "Epoch: 95 | Loss: 0.575 | Accuracy: 0.699 | AUC: 0.542 | Sensitivity: 0.96 | Specificity: 0.125 | Precision: 0.774\n",
      "Epoch: 96 | Loss: 0.572 | Accuracy: 0.702 | AUC: 0.6 | Sensitivity: 1.0 | Specificity: 0.2 | Precision: 0.875\n",
      "Epoch: 97 | Loss: 0.578 | Accuracy: 0.696 | AUC: 0.5 | Sensitivity: 0.909 | Specificity: 0.091 | Precision: 0.667\n",
      "Epoch: 98 | Loss: 0.575 | Accuracy: 0.697 | AUC: 0.523 | Sensitivity: 0.955 | Specificity: 0.091 | Precision: 0.677\n",
      "Epoch: 99 | Loss: 0.574 | Accuracy: 0.7 | AUC: 0.552 | Sensitivity: 0.962 | Specificity: 0.143 | Precision: 0.806\n",
      "Epoch: 100 | Loss: 0.579 | Accuracy: 0.696 | AUC: 0.488 | Sensitivity: 0.9 | Specificity: 0.077 | Precision: 0.6\n",
      "Epoch: 101 | Loss: 0.575 | Accuracy: 0.699 | AUC: 0.578 | Sensitivity: 0.957 | Specificity: 0.2 | Precision: 0.733\n",
      "Epoch: 102 | Loss: 0.575 | Accuracy: 0.699 | AUC: 0.462 | Sensitivity: 0.923 | Specificity: 0.0 | Precision: 0.774\n",
      "Epoch: 103 | Loss: 0.576 | Accuracy: 0.699 | AUC: 0.528 | Sensitivity: 0.957 | Specificity: 0.1 | Precision: 0.71\n",
      "Epoch: 104 | Loss: 0.576 | Accuracy: 0.696 | AUC: 0.545 | Sensitivity: 0.947 | Specificity: 0.143 | Precision: 0.6\n",
      "Epoch: 105 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.636 | Sensitivity: 0.909 | Specificity: 0.364 | Precision: 0.741\n",
      "Epoch: 106 | Loss: 0.578 | Accuracy: 0.694 | AUC: 0.502 | Sensitivity: 0.941 | Specificity: 0.062 | Precision: 0.516\n",
      "Epoch: 107 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.522 | Sensitivity: 0.92 | Specificity: 0.125 | Precision: 0.767\n",
      "Epoch: 108 | Loss: 0.577 | Accuracy: 0.697 | AUC: 0.494 | Sensitivity: 0.905 | Specificity: 0.083 | Precision: 0.633\n",
      "Epoch: 109 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.48 | Sensitivity: 0.96 | Specificity: 0.0 | Precision: 0.75\n",
      "Epoch: 110 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.545 | Sensitivity: 1.0 | Specificity: 0.091 | Precision: 0.688\n",
      "Epoch: 111 | Loss: 0.575 | Accuracy: 0.697 | AUC: 0.475 | Sensitivity: 0.95 | Specificity: 0.0 | Precision: 0.594\n",
      "Epoch: 112 | Loss: 0.574 | Accuracy: 0.697 | AUC: 0.513 | Sensitivity: 0.95 | Specificity: 0.077 | Precision: 0.613\n",
      "Epoch: 113 | Loss: 0.57 | Accuracy: 0.701 | AUC: 0.604 | Sensitivity: 0.923 | Specificity: 0.286 | Precision: 0.828\n",
      "Epoch: 114 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.7\n",
      "Epoch: 115 | Loss: 0.573 | Accuracy: 0.7 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 116 | Loss: 0.57 | Accuracy: 0.701 | AUC: 0.611 | Sensitivity: 1.0 | Specificity: 0.222 | Precision: 0.774\n",
      "Epoch: 117 | Loss: 0.574 | Accuracy: 0.699 | AUC: 0.635 | Sensitivity: 0.842 | Specificity: 0.429 | Precision: 0.667\n",
      "Epoch: 118 | Loss: 0.577 | Accuracy: 0.697 | AUC: 0.533 | Sensitivity: 1.0 | Specificity: 0.067 | Precision: 0.562\n",
      "Epoch: 119 | Loss: 0.574 | Accuracy: 0.695 | AUC: 0.504 | Sensitivity: 0.882 | Specificity: 0.125 | Precision: 0.517\n",
      "Epoch: 120 | Loss: 0.573 | Accuracy: 0.699 | AUC: 0.59 | Sensitivity: 0.95 | Specificity: 0.231 | Precision: 0.655\n",
      "Epoch: 121 | Loss: 0.572 | Accuracy: 0.701 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 122 | Loss: 0.57 | Accuracy: 0.701 | AUC: 0.614 | Sensitivity: 0.955 | Specificity: 0.273 | Precision: 0.724\n",
      "Epoch: 123 | Loss: 0.569 | Accuracy: 0.702 | AUC: 0.636 | Sensitivity: 1.0 | Specificity: 0.273 | Precision: 0.733\n",
      "Epoch: 124 | Loss: 0.568 | Accuracy: 0.704 | AUC: 0.648 | Sensitivity: 0.963 | Specificity: 0.333 | Precision: 0.867\n",
      "Epoch: 125 | Loss: 0.57 | Accuracy: 0.701 | AUC: 0.59 | Sensitivity: 0.958 | Specificity: 0.222 | Precision: 0.767\n",
      "Epoch: 126 | Loss: 0.57 | Accuracy: 0.703 | AUC: 0.571 | Sensitivity: 1.0 | Specificity: 0.143 | Precision: 0.812\n",
      "Epoch: 127 | Loss: 0.569 | Accuracy: 0.702 | AUC: 0.533 | Sensitivity: 0.923 | Specificity: 0.143 | Precision: 0.8\n",
      "Epoch: 128 | Loss: 0.571 | Accuracy: 0.699 | AUC: 0.478 | Sensitivity: 0.957 | Specificity: 0.0 | Precision: 0.688\n",
      "Epoch: 129 | Loss: 0.571 | Accuracy: 0.7 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 130 | Loss: 0.572 | Accuracy: 0.699 | AUC: 0.5 | Sensitivity: 0.909 | Specificity: 0.091 | Precision: 0.667\n",
      "Epoch: 131 | Loss: 0.567 | Accuracy: 0.703 | AUC: 0.646 | Sensitivity: 0.958 | Specificity: 0.333 | Precision: 0.793\n",
      "Epoch: 132 | Loss: 0.569 | Accuracy: 0.701 | AUC: 0.566 | Sensitivity: 0.846 | Specificity: 0.286 | Precision: 0.815\n",
      "Epoch: 133 | Loss: 0.568 | Accuracy: 0.705 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.806\n",
      "Epoch: 134 | Loss: 0.571 | Accuracy: 0.698 | AUC: 0.441 | Sensitivity: 0.783 | Specificity: 0.1 | Precision: 0.667\n",
      "Epoch: 135 | Loss: 0.567 | Accuracy: 0.703 | AUC: 0.552 | Sensitivity: 0.962 | Specificity: 0.143 | Precision: 0.806\n",
      "Epoch: 136 | Loss: 0.568 | Accuracy: 0.703 | AUC: 0.636 | Sensitivity: 1.0 | Specificity: 0.273 | Precision: 0.733\n",
      "Epoch: 137 | Loss: 0.569 | Accuracy: 0.702 | AUC: 0.5 | Sensitivity: 1.0 | Specificity: 0.0 | Precision: 0.727\n",
      "Epoch: 138 | Loss: 0.569 | Accuracy: 0.701 | AUC: 0.535 | Sensitivity: 0.87 | Specificity: 0.2 | Precision: 0.714\n",
      "Epoch: 139 | Loss: 0.567 | Accuracy: 0.705 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.806\n",
      "Epoch: 140 | Loss: 0.567 | Accuracy: 0.704 | AUC: 0.681 | Sensitivity: 0.917 | Specificity: 0.444 | Precision: 0.815\n",
      "Epoch: 141 | Loss: 0.568 | Accuracy: 0.7 | AUC: 0.528 | Sensitivity: 0.833 | Specificity: 0.222 | Precision: 0.741\n",
      "Epoch: 142 | Loss: 0.566 | Accuracy: 0.705 | AUC: 0.648 | Sensitivity: 0.92 | Specificity: 0.375 | Precision: 0.821\n",
      "Epoch: 143 | Loss: 0.569 | Accuracy: 0.701 | AUC: 0.56 | Sensitivity: 0.952 | Specificity: 0.167 | Precision: 0.667\n",
      "Epoch: 144 | Loss: 0.571 | Accuracy: 0.7 | AUC: 0.452 | Sensitivity: 0.905 | Specificity: 0.0 | Precision: 0.613\n",
      "Epoch: 145 | Loss: 0.568 | Accuracy: 0.699 | AUC: 0.47 | Sensitivity: 0.857 | Specificity: 0.083 | Precision: 0.621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146 | Loss: 0.568 | Accuracy: 0.701 | AUC: 0.477 | Sensitivity: 0.955 | Specificity: 0.0 | Precision: 0.656\n",
      "Epoch: 147 | Loss: 0.566 | Accuracy: 0.704 | AUC: 0.6 | Sensitivity: 1.0 | Specificity: 0.2 | Precision: 0.742\n",
      "Epoch: 148 | Loss: 0.569 | Accuracy: 0.702 | AUC: 0.514 | Sensitivity: 0.917 | Specificity: 0.111 | Precision: 0.733\n",
      "Epoch: 149 | Loss: 0.564 | Accuracy: 0.706 | AUC: 0.714 | Sensitivity: 1.0 | Specificity: 0.429 | Precision: 0.867\n",
      "Epoch: 150 | Loss: 0.567 | Accuracy: 0.702 | AUC: 0.457 | Sensitivity: 0.913 | Specificity: 0.0 | Precision: 0.677\n",
      "Epoch: 151 | Loss: 0.568 | Accuracy: 0.702 | AUC: 0.643 | Sensitivity: 1.0 | Specificity: 0.286 | Precision: 0.655\n",
      "Epoch: 152 | Loss: 0.565 | Accuracy: 0.703 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 153 | Loss: 0.565 | Accuracy: 0.704 | AUC: 0.628 | Sensitivity: 0.957 | Specificity: 0.3 | Precision: 0.759\n",
      "Epoch: 154 | Loss: 0.566 | Accuracy: 0.702 | AUC: 0.577 | Sensitivity: 0.905 | Specificity: 0.25 | Precision: 0.679\n",
      "Epoch: 155 | Loss: 0.565 | Accuracy: 0.704 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.7\n",
      "Epoch: 156 | Loss: 0.567 | Accuracy: 0.704 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 157 | Loss: 0.566 | Accuracy: 0.701 | AUC: 0.554 | Sensitivity: 0.8 | Specificity: 0.308 | Precision: 0.64\n",
      "Epoch: 158 | Loss: 0.564 | Accuracy: 0.704 | AUC: 0.542 | Sensitivity: 0.96 | Specificity: 0.125 | Precision: 0.774\n",
      "Epoch: 159 | Loss: 0.566 | Accuracy: 0.701 | AUC: 0.59 | Sensitivity: 0.95 | Specificity: 0.231 | Precision: 0.655\n",
      "Epoch: 160 | Loss: 0.566 | Accuracy: 0.706 | AUC: 0.692 | Sensitivity: 1.0 | Specificity: 0.385 | Precision: 0.714\n",
      "Epoch: 161 | Loss: 0.565 | Accuracy: 0.705 | AUC: 0.696 | Sensitivity: 0.81 | Specificity: 0.583 | Precision: 0.773\n",
      "Epoch: 162 | Loss: 0.564 | Accuracy: 0.703 | AUC: 0.557 | Sensitivity: 0.913 | Specificity: 0.2 | Precision: 0.724\n",
      "Epoch: 163 | Loss: 0.564 | Accuracy: 0.704 | AUC: 0.585 | Sensitivity: 0.87 | Specificity: 0.3 | Precision: 0.741\n",
      "Epoch: 164 | Loss: 0.562 | Accuracy: 0.706 | AUC: 0.556 | Sensitivity: 0.862 | Specificity: 0.25 | Precision: 0.893\n",
      "Epoch: 165 | Loss: 0.565 | Accuracy: 0.704 | AUC: 0.514 | Sensitivity: 0.917 | Specificity: 0.111 | Precision: 0.733\n",
      "Epoch: 166 | Loss: 0.56 | Accuracy: 0.707 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.806\n",
      "Epoch: 167 | Loss: 0.563 | Accuracy: 0.707 | AUC: 0.611 | Sensitivity: 1.0 | Specificity: 0.222 | Precision: 0.774\n",
      "Epoch: 168 | Loss: 0.564 | Accuracy: 0.703 | AUC: 0.512 | Sensitivity: 0.857 | Specificity: 0.167 | Precision: 0.643\n",
      "Epoch: 169 | Loss: 0.562 | Accuracy: 0.708 | AUC: 0.701 | Sensitivity: 0.958 | Specificity: 0.444 | Precision: 0.821\n",
      "Epoch: 170 | Loss: 0.564 | Accuracy: 0.706 | AUC: 0.643 | Sensitivity: 1.0 | Specificity: 0.286 | Precision: 0.655\n",
      "Epoch: 171 | Loss: 0.563 | Accuracy: 0.705 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 172 | Loss: 0.559 | Accuracy: 0.71 | AUC: 0.75 | Sensitivity: 1.0 | Specificity: 0.5 | Precision: 0.862\n",
      "Epoch: 173 | Loss: 0.565 | Accuracy: 0.701 | AUC: 0.507 | Sensitivity: 0.765 | Specificity: 0.25 | Precision: 0.52\n",
      "Epoch: 174 | Loss: 0.563 | Accuracy: 0.705 | AUC: 0.482 | Sensitivity: 0.84 | Specificity: 0.125 | Precision: 0.75\n",
      "Epoch: 175 | Loss: 0.561 | Accuracy: 0.708 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.806\n",
      "Epoch: 176 | Loss: 0.564 | Accuracy: 0.704 | AUC: 0.527 | Sensitivity: 0.9 | Specificity: 0.154 | Precision: 0.621\n",
      "Epoch: 177 | Loss: 0.561 | Accuracy: 0.708 | AUC: 0.605 | Sensitivity: 0.96 | Specificity: 0.25 | Precision: 0.8\n",
      "Epoch: 178 | Loss: 0.562 | Accuracy: 0.706 | AUC: 0.528 | Sensitivity: 0.957 | Specificity: 0.1 | Precision: 0.71\n",
      "Epoch: 179 | Loss: 0.56 | Accuracy: 0.709 | AUC: 0.681 | Sensitivity: 0.917 | Specificity: 0.444 | Precision: 0.815\n",
      "Epoch: 180 | Loss: 0.562 | Accuracy: 0.705 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 181 | Loss: 0.562 | Accuracy: 0.705 | AUC: 0.577 | Sensitivity: 0.905 | Specificity: 0.25 | Precision: 0.679\n",
      "Epoch: 182 | Loss: 0.559 | Accuracy: 0.708 | AUC: 0.678 | Sensitivity: 0.957 | Specificity: 0.4 | Precision: 0.786\n",
      "Epoch: 183 | Loss: 0.56 | Accuracy: 0.708 | AUC: 0.681 | Sensitivity: 0.917 | Specificity: 0.444 | Precision: 0.815\n",
      "Epoch: 184 | Loss: 0.561 | Accuracy: 0.705 | AUC: 0.493 | Sensitivity: 0.875 | Specificity: 0.111 | Precision: 0.724\n",
      "Epoch: 185 | Loss: 0.559 | Accuracy: 0.709 | AUC: 0.701 | Sensitivity: 0.958 | Specificity: 0.444 | Precision: 0.821\n",
      "Epoch: 186 | Loss: 0.562 | Accuracy: 0.704 | AUC: 0.579 | Sensitivity: 0.85 | Specificity: 0.308 | Precision: 0.654\n",
      "Epoch: 187 | Loss: 0.561 | Accuracy: 0.703 | AUC: 0.523 | Sensitivity: 0.864 | Specificity: 0.182 | Precision: 0.679\n",
      "Epoch: 188 | Loss: 0.557 | Accuracy: 0.71 | AUC: 0.757 | Sensitivity: 0.958 | Specificity: 0.556 | Precision: 0.852\n",
      "Epoch: 189 | Loss: 0.559 | Accuracy: 0.708 | AUC: 0.628 | Sensitivity: 0.957 | Specificity: 0.3 | Precision: 0.759\n",
      "Epoch: 190 | Loss: 0.557 | Accuracy: 0.71 | AUC: 0.75 | Sensitivity: 1.0 | Specificity: 0.5 | Precision: 0.862\n",
      "Epoch: 191 | Loss: 0.56 | Accuracy: 0.704 | AUC: 0.42 | Sensitivity: 0.84 | Specificity: 0.0 | Precision: 0.724\n",
      "Epoch: 192 | Loss: 0.56 | Accuracy: 0.704 | AUC: 0.656 | Sensitivity: 1.0 | Specificity: 0.312 | Precision: 0.607\n",
      "Epoch: 193 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.667 | Sensitivity: 1.0 | Specificity: 0.333 | Precision: 0.871\n",
      "Epoch: 194 | Loss: 0.562 | Accuracy: 0.704 | AUC: 0.571 | Sensitivity: 1.0 | Specificity: 0.143 | Precision: 0.613\n",
      "Epoch: 195 | Loss: 0.557 | Accuracy: 0.707 | AUC: 0.59 | Sensitivity: 0.958 | Specificity: 0.222 | Precision: 0.767\n",
      "Epoch: 196 | Loss: 0.56 | Accuracy: 0.705 | AUC: 0.554 | Sensitivity: 0.857 | Specificity: 0.25 | Precision: 0.667\n",
      "Epoch: 197 | Loss: 0.561 | Accuracy: 0.704 | AUC: 0.494 | Sensitivity: 0.905 | Specificity: 0.083 | Precision: 0.633\n",
      "Epoch: 198 | Loss: 0.557 | Accuracy: 0.708 | AUC: 0.569 | Sensitivity: 0.917 | Specificity: 0.222 | Precision: 0.759\n",
      "Epoch: 199 | Loss: 0.557 | Accuracy: 0.708 | AUC: 0.565 | Sensitivity: 0.88 | Specificity: 0.25 | Precision: 0.786\n",
      "Epoch: 200 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.552 | Sensitivity: 0.962 | Specificity: 0.143 | Precision: 0.806\n",
      "Epoch: 201 | Loss: 0.56 | Accuracy: 0.706 | AUC: 0.536 | Sensitivity: 0.905 | Specificity: 0.167 | Precision: 0.655\n",
      "Epoch: 202 | Loss: 0.558 | Accuracy: 0.707 | AUC: 0.583 | Sensitivity: 0.833 | Specificity: 0.333 | Precision: 0.769\n",
      "Epoch: 203 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.533 | Sensitivity: 0.923 | Specificity: 0.143 | Precision: 0.8\n",
      "Epoch: 204 | Loss: 0.559 | Accuracy: 0.707 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 205 | Loss: 0.554 | Accuracy: 0.712 | AUC: 0.778 | Sensitivity: 0.889 | Specificity: 0.667 | Precision: 0.923\n",
      "Epoch: 206 | Loss: 0.559 | Accuracy: 0.708 | AUC: 0.591 | Sensitivity: 1.0 | Specificity: 0.182 | Precision: 0.71\n",
      "Epoch: 207 | Loss: 0.558 | Accuracy: 0.707 | AUC: 0.5 | Sensitivity: 0.909 | Specificity: 0.091 | Precision: 0.667\n",
      "Epoch: 208 | Loss: 0.555 | Accuracy: 0.712 | AUC: 0.664 | Sensitivity: 0.929 | Specificity: 0.4 | Precision: 0.897\n",
      "Epoch: 209 | Loss: 0.563 | Accuracy: 0.703 | AUC: 0.496 | Sensitivity: 0.875 | Specificity: 0.118 | Precision: 0.483\n",
      "Epoch: 210 | Loss: 0.557 | Accuracy: 0.711 | AUC: 0.685 | Sensitivity: 0.952 | Specificity: 0.417 | Precision: 0.741\n",
      "Epoch: 211 | Loss: 0.558 | Accuracy: 0.709 | AUC: 0.637 | Sensitivity: 0.857 | Specificity: 0.417 | Precision: 0.72\n",
      "Epoch: 212 | Loss: 0.56 | Accuracy: 0.709 | AUC: 0.458 | Sensitivity: 0.917 | Specificity: 0.0 | Precision: 0.71\n",
      "Epoch: 213 | Loss: 0.557 | Accuracy: 0.707 | AUC: 0.476 | Sensitivity: 0.952 | Specificity: 0.0 | Precision: 0.625\n",
      "Epoch: 214 | Loss: 0.559 | Accuracy: 0.707 | AUC: 0.47 | Sensitivity: 0.739 | Specificity: 0.2 | Precision: 0.68\n",
      "Epoch: 215 | Loss: 0.554 | Accuracy: 0.715 | AUC: 0.731 | Sensitivity: 0.963 | Specificity: 0.5 | Precision: 0.897\n",
      "Epoch: 216 | Loss: 0.557 | Accuracy: 0.707 | AUC: 0.506 | Sensitivity: 0.762 | Specificity: 0.25 | Precision: 0.64\n",
      "Epoch: 217 | Loss: 0.558 | Accuracy: 0.709 | AUC: 0.542 | Sensitivity: 1.0 | Specificity: 0.083 | Precision: 0.656\n",
      "Epoch: 218 | Loss: 0.555 | Accuracy: 0.711 | AUC: 0.542 | Sensitivity: 0.96 | Specificity: 0.125 | Precision: 0.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219 | Loss: 0.557 | Accuracy: 0.711 | AUC: 0.48 | Sensitivity: 0.96 | Specificity: 0.0 | Precision: 0.75\n",
      "Epoch: 220 | Loss: 0.554 | Accuracy: 0.711 | AUC: 0.585 | Sensitivity: 0.92 | Specificity: 0.25 | Precision: 0.793\n",
      "Epoch: 221 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 222 | Loss: 0.556 | Accuracy: 0.71 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 223 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.5 | Sensitivity: 0.909 | Specificity: 0.091 | Precision: 0.667\n",
      "Epoch: 224 | Loss: 0.557 | Accuracy: 0.708 | AUC: 0.581 | Sensitivity: 0.947 | Specificity: 0.214 | Precision: 0.621\n",
      "Epoch: 225 | Loss: 0.554 | Accuracy: 0.712 | AUC: 0.569 | Sensitivity: 0.917 | Specificity: 0.222 | Precision: 0.759\n",
      "Epoch: 226 | Loss: 0.555 | Accuracy: 0.711 | AUC: 0.583 | Sensitivity: 0.833 | Specificity: 0.333 | Precision: 0.769\n",
      "Epoch: 227 | Loss: 0.552 | Accuracy: 0.713 | AUC: 0.605 | Sensitivity: 0.96 | Specificity: 0.25 | Precision: 0.8\n",
      "Epoch: 228 | Loss: 0.552 | Accuracy: 0.713 | AUC: 0.646 | Sensitivity: 0.958 | Specificity: 0.333 | Precision: 0.793\n",
      "Epoch: 229 | Loss: 0.553 | Accuracy: 0.712 | AUC: 0.607 | Sensitivity: 0.913 | Specificity: 0.3 | Precision: 0.75\n",
      "Epoch: 230 | Loss: 0.556 | Accuracy: 0.71 | AUC: 0.606 | Sensitivity: 0.944 | Specificity: 0.267 | Precision: 0.607\n",
      "Epoch: 231 | Loss: 0.555 | Accuracy: 0.71 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 232 | Loss: 0.551 | Accuracy: 0.712 | AUC: 0.5 | Sensitivity: 1.0 | Specificity: 0.0 | Precision: 0.727\n",
      "Epoch: 233 | Loss: 0.552 | Accuracy: 0.715 | AUC: 0.667 | Sensitivity: 1.0 | Specificity: 0.333 | Precision: 0.8\n",
      "Epoch: 234 | Loss: 0.556 | Accuracy: 0.709 | AUC: 0.488 | Sensitivity: 0.9 | Specificity: 0.077 | Precision: 0.6\n",
      "Epoch: 235 | Loss: 0.554 | Accuracy: 0.713 | AUC: 0.65 | Sensitivity: 1.0 | Specificity: 0.3 | Precision: 0.767\n",
      "Epoch: 236 | Loss: 0.552 | Accuracy: 0.711 | AUC: 0.55 | Sensitivity: 1.0 | Specificity: 0.1 | Precision: 0.719\n",
      "Epoch: 237 | Loss: 0.551 | Accuracy: 0.713 | AUC: 0.659 | Sensitivity: 0.955 | Specificity: 0.364 | Precision: 0.75\n",
      "Epoch: 238 | Loss: 0.552 | Accuracy: 0.713 | AUC: 0.604 | Sensitivity: 0.875 | Specificity: 0.333 | Precision: 0.778\n",
      "Epoch: 239 | Loss: 0.553 | Accuracy: 0.713 | AUC: 0.562 | Sensitivity: 1.0 | Specificity: 0.125 | Precision: 0.781\n",
      "Epoch: 240 | Loss: 0.553 | Accuracy: 0.712 | AUC: 0.583 | Sensitivity: 1.0 | Specificity: 0.167 | Precision: 0.677\n",
      "Epoch: 241 | Loss: 0.554 | Accuracy: 0.716 | AUC: 0.727 | Sensitivity: 1.0 | Specificity: 0.455 | Precision: 0.786\n",
      "Epoch: 242 | Loss: 0.55 | Accuracy: 0.717 | AUC: 0.714 | Sensitivity: 1.0 | Specificity: 0.429 | Precision: 0.867\n",
      "Epoch: 243 | Loss: 0.553 | Accuracy: 0.715 | AUC: 0.75 | Sensitivity: 1.0 | Specificity: 0.5 | Precision: 0.778\n",
      "Epoch: 244 | Loss: 0.552 | Accuracy: 0.712 | AUC: 0.585 | Sensitivity: 0.87 | Specificity: 0.3 | Precision: 0.741\n",
      "Epoch: 245 | Loss: 0.552 | Accuracy: 0.713 | AUC: 0.481 | Sensitivity: 0.962 | Specificity: 0.0 | Precision: 0.781\n",
      "Epoch: 246 | Loss: 0.551 | Accuracy: 0.714 | AUC: 0.66 | Sensitivity: 0.875 | Specificity: 0.444 | Precision: 0.808\n",
      "Epoch: 247 | Loss: 0.553 | Accuracy: 0.71 | AUC: 0.485 | Sensitivity: 0.87 | Specificity: 0.1 | Precision: 0.69\n",
      "Epoch: 248 | Loss: 0.554 | Accuracy: 0.709 | AUC: 0.488 | Sensitivity: 0.9 | Specificity: 0.077 | Precision: 0.6\n",
      "Epoch: 249 | Loss: 0.552 | Accuracy: 0.713 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 250 | Loss: 0.548 | Accuracy: 0.715 | AUC: 0.582 | Sensitivity: 0.964 | Specificity: 0.2 | Precision: 0.871\n",
      "Epoch: 251 | Loss: 0.553 | Accuracy: 0.713 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 252 | Loss: 0.556 | Accuracy: 0.707 | AUC: 0.428 | Sensitivity: 0.722 | Specificity: 0.133 | Precision: 0.5\n",
      "Epoch: 253 | Loss: 0.553 | Accuracy: 0.711 | AUC: 0.545 | Sensitivity: 0.947 | Specificity: 0.143 | Precision: 0.6\n",
      "Epoch: 254 | Loss: 0.551 | Accuracy: 0.713 | AUC: 0.619 | Sensitivity: 0.905 | Specificity: 0.333 | Precision: 0.704\n",
      "Epoch: 255 | Loss: 0.552 | Accuracy: 0.711 | AUC: 0.523 | Sensitivity: 0.864 | Specificity: 0.182 | Precision: 0.679\n",
      "Epoch: 256 | Loss: 0.552 | Accuracy: 0.714 | AUC: 0.614 | Sensitivity: 0.864 | Specificity: 0.364 | Precision: 0.731\n",
      "Epoch: 257 | Loss: 0.553 | Accuracy: 0.712 | AUC: 0.552 | Sensitivity: 0.95 | Specificity: 0.154 | Precision: 0.633\n",
      "Epoch: 258 | Loss: 0.553 | Accuracy: 0.71 | AUC: 0.509 | Sensitivity: 0.947 | Specificity: 0.071 | Precision: 0.581\n",
      "Epoch: 259 | Loss: 0.551 | Accuracy: 0.711 | AUC: 0.435 | Sensitivity: 0.87 | Specificity: 0.0 | Precision: 0.667\n",
      "Epoch: 260 | Loss: 0.548 | Accuracy: 0.716 | AUC: 0.678 | Sensitivity: 0.957 | Specificity: 0.4 | Precision: 0.786\n",
      "Epoch: 261 | Loss: 0.551 | Accuracy: 0.713 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 262 | Loss: 0.548 | Accuracy: 0.714 | AUC: 0.643 | Sensitivity: 0.952 | Specificity: 0.333 | Precision: 0.714\n",
      "Epoch: 263 | Loss: 0.55 | Accuracy: 0.718 | AUC: 0.646 | Sensitivity: 0.958 | Specificity: 0.333 | Precision: 0.793\n",
      "Epoch: 264 | Loss: 0.55 | Accuracy: 0.713 | AUC: 0.604 | Sensitivity: 0.9 | Specificity: 0.308 | Precision: 0.667\n",
      "Epoch: 265 | Loss: 0.551 | Accuracy: 0.713 | AUC: 0.485 | Sensitivity: 0.87 | Specificity: 0.1 | Precision: 0.69\n",
      "Epoch: 266 | Loss: 0.547 | Accuracy: 0.717 | AUC: 0.773 | Sensitivity: 0.909 | Specificity: 0.636 | Precision: 0.833\n",
      "Epoch: 267 | Loss: 0.546 | Accuracy: 0.717 | AUC: 0.624 | Sensitivity: 0.962 | Specificity: 0.286 | Precision: 0.833\n",
      "Epoch: 268 | Loss: 0.551 | Accuracy: 0.712 | AUC: 0.54 | Sensitivity: 0.85 | Specificity: 0.231 | Precision: 0.63\n",
      "Epoch: 269 | Loss: 0.548 | Accuracy: 0.716 | AUC: 0.643 | Sensitivity: 0.952 | Specificity: 0.333 | Precision: 0.714\n",
      "Epoch: 270 | Loss: 0.546 | Accuracy: 0.719 | AUC: 0.695 | Sensitivity: 0.962 | Specificity: 0.429 | Precision: 0.862\n",
      "Epoch: 271 | Loss: 0.547 | Accuracy: 0.718 | AUC: 0.611 | Sensitivity: 0.889 | Specificity: 0.333 | Precision: 0.857\n",
      "Epoch: 272 | Loss: 0.546 | Accuracy: 0.719 | AUC: 0.75 | Sensitivity: 1.0 | Specificity: 0.5 | Precision: 0.821\n",
      "Epoch: 273 | Loss: 0.548 | Accuracy: 0.715 | AUC: 0.522 | Sensitivity: 0.92 | Specificity: 0.125 | Precision: 0.767\n",
      "Epoch: 274 | Loss: 0.545 | Accuracy: 0.719 | AUC: 0.688 | Sensitivity: 1.0 | Specificity: 0.375 | Precision: 0.833\n",
      "Epoch: 275 | Loss: 0.55 | Accuracy: 0.714 | AUC: 0.46 | Sensitivity: 0.92 | Specificity: 0.0 | Precision: 0.742\n",
      "Epoch: 276 | Loss: 0.546 | Accuracy: 0.715 | AUC: 0.607 | Sensitivity: 0.913 | Specificity: 0.3 | Precision: 0.75\n",
      "Epoch: 277 | Loss: 0.546 | Accuracy: 0.715 | AUC: 0.557 | Sensitivity: 0.913 | Specificity: 0.2 | Precision: 0.724\n",
      "Epoch: 278 | Loss: 0.548 | Accuracy: 0.714 | AUC: 0.565 | Sensitivity: 0.9 | Specificity: 0.231 | Precision: 0.643\n",
      "Epoch: 279 | Loss: 0.549 | Accuracy: 0.712 | AUC: 0.557 | Sensitivity: 0.938 | Specificity: 0.176 | Precision: 0.517\n",
      "Epoch: 280 | Loss: 0.545 | Accuracy: 0.718 | AUC: 0.59 | Sensitivity: 0.958 | Specificity: 0.222 | Precision: 0.767\n",
      "Epoch: 281 | Loss: 0.547 | Accuracy: 0.716 | AUC: 0.585 | Sensitivity: 0.87 | Specificity: 0.3 | Precision: 0.741\n",
      "Epoch: 282 | Loss: 0.544 | Accuracy: 0.717 | AUC: 0.611 | Sensitivity: 0.889 | Specificity: 0.333 | Precision: 0.857\n",
      "Epoch: 283 | Loss: 0.547 | Accuracy: 0.719 | AUC: 0.682 | Sensitivity: 1.0 | Specificity: 0.364 | Precision: 0.759\n",
      "Epoch: 284 | Loss: 0.548 | Accuracy: 0.716 | AUC: 0.619 | Sensitivity: 0.905 | Specificity: 0.333 | Precision: 0.704\n",
      "Epoch: 285 | Loss: 0.545 | Accuracy: 0.717 | AUC: 0.607 | Sensitivity: 0.84 | Specificity: 0.375 | Precision: 0.808\n",
      "Epoch: 286 | Loss: 0.545 | Accuracy: 0.718 | AUC: 0.566 | Sensitivity: 0.846 | Specificity: 0.286 | Precision: 0.815\n",
      "Epoch: 287 | Loss: 0.548 | Accuracy: 0.716 | AUC: 0.615 | Sensitivity: 1.0 | Specificity: 0.231 | Precision: 0.667\n",
      "Epoch: 288 | Loss: 0.549 | Accuracy: 0.714 | AUC: 0.511 | Sensitivity: 0.889 | Specificity: 0.133 | Precision: 0.552\n",
      "Epoch: 289 | Loss: 0.547 | Accuracy: 0.716 | AUC: 0.525 | Sensitivity: 0.8 | Specificity: 0.25 | Precision: 0.769\n",
      "Epoch: 290 | Loss: 0.548 | Accuracy: 0.716 | AUC: 0.555 | Sensitivity: 0.895 | Specificity: 0.214 | Precision: 0.607\n",
      "Epoch: 291 | Loss: 0.543 | Accuracy: 0.721 | AUC: 0.7 | Sensitivity: 1.0 | Specificity: 0.4 | Precision: 0.793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 292 | Loss: 0.545 | Accuracy: 0.719 | AUC: 0.648 | Sensitivity: 0.92 | Specificity: 0.375 | Precision: 0.821\n",
      "Epoch: 293 | Loss: 0.546 | Accuracy: 0.718 | AUC: 0.563 | Sensitivity: 0.826 | Specificity: 0.3 | Precision: 0.731\n",
      "Epoch: 294 | Loss: 0.546 | Accuracy: 0.719 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.7\n",
      "Epoch: 295 | Loss: 0.546 | Accuracy: 0.717 | AUC: 0.442 | Sensitivity: 0.76 | Specificity: 0.125 | Precision: 0.731\n",
      "Epoch: 296 | Loss: 0.547 | Accuracy: 0.717 | AUC: 0.513 | Sensitivity: 0.95 | Specificity: 0.077 | Precision: 0.613\n",
      "Epoch: 297 | Loss: 0.546 | Accuracy: 0.717 | AUC: 0.56 | Sensitivity: 0.952 | Specificity: 0.167 | Precision: 0.667\n",
      "Epoch: 298 | Loss: 0.547 | Accuracy: 0.718 | AUC: 0.56 | Sensitivity: 0.952 | Specificity: 0.167 | Precision: 0.667\n",
      "Epoch: 299 | Loss: 0.544 | Accuracy: 0.721 | AUC: 0.546 | Sensitivity: 0.893 | Specificity: 0.2 | Precision: 0.862\n",
      "Epoch: 300 | Loss: 0.543 | Accuracy: 0.724 | AUC: 0.688 | Sensitivity: 1.0 | Specificity: 0.375 | Precision: 0.833\n",
      "Epoch: 301 | Loss: 0.549 | Accuracy: 0.718 | AUC: 0.5 | Sensitivity: 1.0 | Specificity: 0.0 | Precision: 0.667\n",
      "Epoch: 302 | Loss: 0.546 | Accuracy: 0.717 | AUC: 0.545 | Sensitivity: 0.909 | Specificity: 0.182 | Precision: 0.69\n",
      "Epoch: 303 | Loss: 0.547 | Accuracy: 0.719 | AUC: 0.583 | Sensitivity: 0.833 | Specificity: 0.333 | Precision: 0.769\n",
      "Epoch: 304 | Loss: 0.542 | Accuracy: 0.722 | AUC: 0.646 | Sensitivity: 0.958 | Specificity: 0.333 | Precision: 0.793\n",
      "Epoch: 305 | Loss: 0.544 | Accuracy: 0.721 | AUC: 0.667 | Sensitivity: 1.0 | Specificity: 0.333 | Precision: 0.724\n",
      "Epoch: 306 | Loss: 0.541 | Accuracy: 0.723 | AUC: 0.625 | Sensitivity: 1.0 | Specificity: 0.25 | Precision: 0.806\n",
      "Epoch: 307 | Loss: 0.542 | Accuracy: 0.724 | AUC: 0.688 | Sensitivity: 1.0 | Specificity: 0.375 | Precision: 0.833\n",
      "Epoch: 308 | Loss: 0.543 | Accuracy: 0.72 | AUC: 0.657 | Sensitivity: 0.913 | Specificity: 0.4 | Precision: 0.778\n",
      "Epoch: 309 | Loss: 0.544 | Accuracy: 0.719 | AUC: 0.601 | Sensitivity: 0.952 | Specificity: 0.25 | Precision: 0.69\n",
      "Epoch: 310 | Loss: 0.545 | Accuracy: 0.72 | AUC: 0.711 | Sensitivity: 0.889 | Specificity: 0.533 | Precision: 0.696\n",
      "Epoch: 311 | Loss: 0.541 | Accuracy: 0.723 | AUC: 0.556 | Sensitivity: 1.0 | Specificity: 0.111 | Precision: 0.75\n",
      "Epoch: 312 | Loss: 0.543 | Accuracy: 0.718 | AUC: 0.592 | Sensitivity: 0.8 | Specificity: 0.385 | Precision: 0.667\n",
      "Epoch: 313 | Loss: 0.54 | Accuracy: 0.722 | AUC: 0.66 | Sensitivity: 0.875 | Specificity: 0.444 | Precision: 0.808\n",
      "Epoch: 314 | Loss: 0.543 | Accuracy: 0.722 | AUC: 0.724 | Sensitivity: 0.947 | Specificity: 0.5 | Precision: 0.72\n",
      "Epoch: 315 | Loss: 0.541 | Accuracy: 0.723 | AUC: 0.509 | Sensitivity: 0.852 | Specificity: 0.167 | Precision: 0.821\n",
      "Epoch: 316 | Loss: 0.543 | Accuracy: 0.721 | AUC: 0.629 | Sensitivity: 0.95 | Specificity: 0.308 | Precision: 0.679\n",
      "Epoch: 317 | Loss: 0.542 | Accuracy: 0.721 | AUC: 0.537 | Sensitivity: 0.741 | Specificity: 0.333 | Precision: 0.833\n",
      "Epoch: 318 | Loss: 0.541 | Accuracy: 0.723 | AUC: 0.682 | Sensitivity: 0.909 | Specificity: 0.455 | Precision: 0.769\n",
      "Epoch: 319 | Loss: 0.542 | Accuracy: 0.721 | AUC: 0.549 | Sensitivity: 0.875 | Specificity: 0.222 | Precision: 0.75\n",
      "Epoch: 320 | Loss: 0.545 | Accuracy: 0.719 | AUC: 0.455 | Sensitivity: 0.818 | Specificity: 0.091 | Precision: 0.643\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "#turn training on \n",
    "net.train()\n",
    "\n",
    "#track loss\n",
    "losses = []\n",
    "aucs= []\n",
    "\n",
    "#loop through epochs\n",
    "for e in range(1,num_epochs+1):\n",
    "    \n",
    "    #setup evaluation metrics\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_auc = 0\n",
    "    epoch_sens = 0\n",
    "    epoch_spec = 0\n",
    "    epoch_prec = 0\n",
    "    \n",
    "    #loop through batches\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        X_batch = batch[0].float()\n",
    "        y_batch = batch[1].float()\n",
    "        #zero the gradient\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        #predict the output\n",
    "        y_batch_pred = net(X_batch)\n",
    "        \n",
    "        #calculate the loss (check expected function dimensions)\n",
    "        loss = criterion(y_batch_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        #calculate the accuracy\n",
    "        acc = binary_acc(y_batch_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        #calculate auc, sensitivity, specificity and precision\n",
    "        auc, sens, spec, prec = confusion_metrics(y_batch_pred, y_batch)\n",
    "        \n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #update the weights\n",
    "        opt.step()\n",
    "        \n",
    "        #accumulate losses\n",
    "        losses.append(loss.item())\n",
    "        aucs.append(auc)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_auc += auc\n",
    "        epoch_sens += sens\n",
    "        epoch_spec += spec\n",
    "        epoch_prec += prec\n",
    "    \n",
    "    #present epoch outputs\n",
    "    print(\"Epoch: \" + str(e) + \" | Loss: \" + str(round(epoch_loss / len(train_loader), 3)) \\\n",
    "          + \" | Accuracy: \" + str(round(epoch_acc / len(train_loader), 3)) \\\n",
    "        + \" | AUC: \" + str(round(auc,3)) \\\n",
    "         + \" | Sensitivity: \" + str(round(sens,3)) \\\n",
    "         + \" | Specificity: \" + str(round(spec,3)) \\\n",
    "         + \" | Precision: \" + str(round(prec,3)) \n",
    "         )\n",
    "\n",
    "#Next steps \n",
    "    #-> build evaluation to test accuracy of predictions\n",
    "    #-> review if setup correct / how to add improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.lines.Line2D object at 0x7f86b7e666d0>]\n",
      "[<matplotlib.lines.Line2D object at 0x7f86b7e66b50>]\n",
      "0.5666184642121345\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#consider adding smoothing\n",
    "print(plt.plot(losses))\n",
    "\n",
    "print(plt.plot(aucs))\n",
    "\n",
    "print(np.mean(aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_prs(prs, label):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(prs)\n",
    "        output = torch.round(output)\n",
    "        print(\"Pred > \", output.item())\n",
    "        print(\"Label = \", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  0.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  0.0\n",
      "Pred >  0.0\n",
      "Label =  0.0\n",
      "Pred >  0.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  0.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n",
      "Pred >  1.0\n",
      "Label =  1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "    prs = x_train[i].float()\n",
    "    label = y_train[i].float()\n",
    "    evaluate_random_prs(prs, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 | Accuracy: 0.64 | AUC: 0.53 | Sensitivity: 0.825 | Specificity: 0.234 | Precision: 0.702\n",
      "Batch: 1 | Accuracy: 0.66 | AUC: 0.529 | Sensitivity: 0.892 | Specificity: 0.167 | Precision: 0.695\n",
      "Batch: 2 | Accuracy: 0.673 | AUC: 0.534 | Sensitivity: 0.922 | Specificity: 0.146 | Precision: 0.696\n",
      "Batch: 3 | Accuracy: 0.62 | AUC: 0.503 | Sensitivity: 0.869 | Specificity: 0.137 | Precision: 0.662\n",
      "Batch: 4 | Accuracy: 0.68 | AUC: 0.533 | Sensitivity: 0.941 | Specificity: 0.125 | Precision: 0.696\n",
      "Batch: 5 | Accuracy: 0.68 | AUC: 0.57 | Sensitivity: 0.9 | Specificity: 0.24 | Precision: 0.703\n",
      "Batch: 6 | Accuracy: 0.673 | AUC: 0.531 | Sensitivity: 0.836 | Specificity: 0.225 | Precision: 0.748\n",
      "Batch: 7 | Accuracy: 0.62 | AUC: 0.528 | Sensitivity: 0.874 | Specificity: 0.182 | Precision: 0.648\n",
      "Batch: 8 | Accuracy: 0.707 | AUC: 0.534 | Sensitivity: 0.926 | Specificity: 0.143 | Precision: 0.735\n",
      "Batch: 9 | Accuracy: 0.707 | AUC: 0.572 | Sensitivity: 0.888 | Specificity: 0.256 | Precision: 0.748\n",
      "Batch: 10 | Accuracy: 0.705 | AUC: 0.57 | Sensitivity: 0.929 | Specificity: 0.211 | Precision: 0.722\n"
     ]
    }
   ],
   "source": [
    "#full test set evaluation\n",
    "\n",
    "test_acc = []\n",
    "test_auc = []\n",
    "test_sens = []\n",
    "test_spec = []\n",
    "test_prec = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch in enumerate(test_loader):\n",
    "        X_batch = batch[0].float()\n",
    "        y_batch = batch[1].float()\n",
    "\n",
    "        #predict the output\n",
    "        y_batch_pred = net(X_batch)\n",
    "\n",
    "        #calculate the accuracy\n",
    "        acc = binary_acc(y_batch_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "        #calculate auc, sensitivity, specificity and precision\n",
    "        auc, sens, spec, prec = confusion_metrics(y_batch_pred, y_batch)\n",
    "\n",
    "        test_acc.append(acc.item())\n",
    "        test_auc.append(auc)\n",
    "        test_sens.append(sens)\n",
    "        test_spec.append(spec)\n",
    "        test_prec.append(prec)\n",
    "\n",
    "        #present batch outputs\n",
    "        print(\"Batch: \" + str(batch_index) \\\n",
    "              + \" | Accuracy: \" + str(round(acc.item(), 3)) \\\n",
    "              + \" | AUC: \" + str(round(auc,3)) \\\n",
    "              + \" | Sensitivity: \" + str(round(sens,3)) \\\n",
    "              + \" | Specificity: \" + str(round(spec,3)) \\\n",
    "              + \" | Precision: \" + str(round(prec,3)) \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.6695380048318342\n",
      "Mean auc:  0.5393654666227389\n",
      "Mean sensitivity:  0.8910206560540737\n",
      "Mean specificity:  0.187710277191404\n",
      "Mean precision:  0.7050636886134058\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy: \", np.mean(test_acc))\n",
    "print(\"Mean auc: \", np.mean(test_auc))\n",
    "print(\"Mean sensitivity: \", np.mean(test_sens))\n",
    "print(\"Mean specificity: \", np.mean(test_spec))\n",
    "print(\"Mean precision: \", np.mean(test_prec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
