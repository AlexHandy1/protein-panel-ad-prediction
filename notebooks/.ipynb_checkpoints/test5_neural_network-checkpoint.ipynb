{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Case</th>\n",
       "      <th>Sex</th>\n",
       "      <th>C4A.C4B.4481.34.2</th>\n",
       "      <th>IL10.2773.50.2</th>\n",
       "      <th>MMP9.2579.17.5</th>\n",
       "      <th>CSF3.8952.65.3</th>\n",
       "      <th>GC.6581.50.3</th>\n",
       "      <th>APOB.2797.56.2</th>\n",
       "      <th>CFH.4159.130.1</th>\n",
       "      <th>...</th>\n",
       "      <th>C3.2755.8.2</th>\n",
       "      <th>PPY.4588.1.2</th>\n",
       "      <th>IGFBP2.2570.72.5</th>\n",
       "      <th>APOE.2937.10.2</th>\n",
       "      <th>FGA.FGB.FGG.4907.56.1</th>\n",
       "      <th>PLG.3710.49.2</th>\n",
       "      <th>TNF.5936.53.3</th>\n",
       "      <th>ANGPT2.13660.76.3</th>\n",
       "      <th>CRP.4337.49.2</th>\n",
       "      <th>VCAM1.2967.8.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.069435</td>\n",
       "      <td>-0.034924</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.351996</td>\n",
       "      <td>0.314848</td>\n",
       "      <td>-0.318171</td>\n",
       "      <td>0.195647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057947</td>\n",
       "      <td>0.749855</td>\n",
       "      <td>0.146249</td>\n",
       "      <td>-0.133883</td>\n",
       "      <td>-2.490502</td>\n",
       "      <td>-0.324963</td>\n",
       "      <td>-0.978661</td>\n",
       "      <td>0.039893</td>\n",
       "      <td>-0.266216</td>\n",
       "      <td>-0.038887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.071661</td>\n",
       "      <td>-0.421213</td>\n",
       "      <td>-0.872182</td>\n",
       "      <td>0.101832</td>\n",
       "      <td>-0.296282</td>\n",
       "      <td>0.689710</td>\n",
       "      <td>-0.409555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.903393</td>\n",
       "      <td>0.306309</td>\n",
       "      <td>-1.152134</td>\n",
       "      <td>0.421410</td>\n",
       "      <td>0.405951</td>\n",
       "      <td>0.950770</td>\n",
       "      <td>1.549364</td>\n",
       "      <td>1.410669</td>\n",
       "      <td>-1.659654</td>\n",
       "      <td>2.214628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321382</td>\n",
       "      <td>-2.000421</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.952631</td>\n",
       "      <td>-2.252077</td>\n",
       "      <td>-1.264962</td>\n",
       "      <td>...</td>\n",
       "      <td>1.754089</td>\n",
       "      <td>1.248028</td>\n",
       "      <td>1.187176</td>\n",
       "      <td>-1.003851</td>\n",
       "      <td>-0.848056</td>\n",
       "      <td>-0.324963</td>\n",
       "      <td>-1.076009</td>\n",
       "      <td>0.381713</td>\n",
       "      <td>-0.420168</td>\n",
       "      <td>-0.413070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.575056</td>\n",
       "      <td>-0.256073</td>\n",
       "      <td>0.618608</td>\n",
       "      <td>-1.863235</td>\n",
       "      <td>-0.379302</td>\n",
       "      <td>0.689710</td>\n",
       "      <td>-0.636036</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.259195</td>\n",
       "      <td>-0.329593</td>\n",
       "      <td>-0.662532</td>\n",
       "      <td>-0.133883</td>\n",
       "      <td>1.754016</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-1.708552</td>\n",
       "      <td>-1.134225</td>\n",
       "      <td>-1.053209</td>\n",
       "      <td>-0.110263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.512781</td>\n",
       "      <td>0.417969</td>\n",
       "      <td>-0.126787</td>\n",
       "      <td>0.091216</td>\n",
       "      <td>-0.337681</td>\n",
       "      <td>-0.318171</td>\n",
       "      <td>-0.115083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554612</td>\n",
       "      <td>-0.582945</td>\n",
       "      <td>0.512406</td>\n",
       "      <td>0.843965</td>\n",
       "      <td>0.379672</td>\n",
       "      <td>-1.239034</td>\n",
       "      <td>0.858960</td>\n",
       "      <td>-0.683777</td>\n",
       "      <td>-0.344878</td>\n",
       "      <td>-0.487496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group  Case  Sex  C4A.C4B.4481.34.2  IL10.2773.50.2  MMP9.2579.17.5  \\\n",
       "0     A     2    2          -0.069435       -0.034924       -0.126787   \n",
       "1     A     2    2          -1.071661       -0.421213       -0.872182   \n",
       "2     A     2    1           0.321382       -2.000421       -0.126787   \n",
       "3     A     2    2           0.575056       -0.256073        0.618608   \n",
       "4     A     2    2           1.512781        0.417969       -0.126787   \n",
       "\n",
       "   CSF3.8952.65.3  GC.6581.50.3  APOB.2797.56.2  CFH.4159.130.1  ...  \\\n",
       "0        0.351996      0.314848       -0.318171        0.195647  ...   \n",
       "1        0.101832     -0.296282        0.689710       -0.409555  ...   \n",
       "2        0.207792      0.952631       -2.252077       -1.264962  ...   \n",
       "3       -1.863235     -0.379302        0.689710       -0.636036  ...   \n",
       "4        0.091216     -0.337681       -0.318171       -0.115083  ...   \n",
       "\n",
       "   C3.2755.8.2  PPY.4588.1.2  IGFBP2.2570.72.5  APOE.2937.10.2  \\\n",
       "0     0.057947      0.749855          0.146249       -0.133883   \n",
       "1    -0.903393      0.306309         -1.152134        0.421410   \n",
       "2     1.754089      1.248028          1.187176       -1.003851   \n",
       "3    -1.259195     -0.329593         -0.662532       -0.133883   \n",
       "4     0.554612     -0.582945          0.512406        0.843965   \n",
       "\n",
       "   FGA.FGB.FGG.4907.56.1  PLG.3710.49.2  TNF.5936.53.3  ANGPT2.13660.76.3  \\\n",
       "0              -2.490502      -0.324963      -0.978661           0.039893   \n",
       "1               0.405951       0.950770       1.549364           1.410669   \n",
       "2              -0.848056      -0.324963      -1.076009           0.381713   \n",
       "3               1.754016       0.614542      -1.708552          -1.134225   \n",
       "4               0.379672      -1.239034       0.858960          -0.683777   \n",
       "\n",
       "   CRP.4337.49.2  VCAM1.2967.8.1  \n",
       "0      -0.266216       -0.038887  \n",
       "1      -1.659654        2.214628  \n",
       "2      -0.420168       -0.413070  \n",
       "3      -1.053209       -0.110263  \n",
       "4      -0.344878       -0.487496  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data processed on the HPC\n",
    "\n",
    "prs = pd.read_csv(\"../data/protein_prs_cases.csv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "prs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Case          Sex\n",
      "count  6244.000000  6244.000000\n",
      "mean      0.686579     0.598334\n",
      "std       0.463921     0.490274\n",
      "min       0.000000     0.000000\n",
      "25%       0.000000     0.000000\n",
      "50%       1.000000     1.000000\n",
      "75%       1.000000     1.000000\n",
      "max       1.000000     1.000000\n",
      "Group\n",
      "A    3277\n",
      "B     639\n",
      "C     371\n",
      "Name: Case, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Update case and sex from 2/1 and to dummy variables 1/0\n",
    "#Case -> 1 = \"AD\", 0 = \"CTL\"\n",
    "#Sex -> 1 = \"Female\", 0 = \"Male\"\n",
    "\n",
    "if (prs[\"Case\"].max() == 2) | (prs[\"Sex\"].max() == 2):\n",
    "    prs.loc[prs[\"Case\"] == 1, \"Case\"] = 0\n",
    "    prs.loc[prs[\"Case\"] == 2, \"Case\"] = 1\n",
    "\n",
    "    prs.loc[prs[\"Sex\"] == 1, \"Sex\"] = 0\n",
    "    prs.loc[prs[\"Sex\"] == 2, \"Sex\"] = 1\n",
    "else:\n",
    "    print(\"Already updated\")\n",
    "\n",
    "print(prs[[\"Case\", \"Sex\"]].describe())\n",
    "\n",
    "print(prs.groupby(\"Group\")[\"Case\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOAL - predict AD status from 31 proteins using a simple feed forward neural network\n",
    "\n",
    "#Questions\n",
    "#-> how represent the input proteins e.g. float vector of 31 values? What dimensions?\n",
    "#-> how represent the output of AD disease status e.g. a 2 node output layer connected by a softmax / sigmoid function? \n",
    "#-> how structure hidden layers? How many?\n",
    "#-> how initialise layers?\n",
    "#-> what objective, loss function?\n",
    "#-> how setup batch sizes for training? e.g. all samples in one batch, randomised sub-samples, 1 sample at a time?\n",
    "    #-> are batches all calculated at once? e.g. [10, 31] input if batch of 10\n",
    "#-> when do you need to explicitly set a model to train? (varied across examples, not in RNN, is in CNN images)\n",
    "\n",
    "#Resources\n",
    "#-> binary classifier gist - https://gist.github.com/santi-pdp/d0e9002afe74db04aa5bbff6d076e8fe\n",
    "#-> binary cross entropy loss - https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a \n",
    "#-> practical stack overflow example - https://stackoverflow.com/questions/62413462/why-is-a-simple-binary-classification-failing-in-a-feedforward-neural-network\n",
    "#-> microsoft binary classifier example (+ code) - https://visualstudiomagazine.com/articles/2020/10/14/pytorch-define-network.aspx\n",
    "#-> more detailed analytics vidya example - https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/\n",
    "#-> MNIST examples - https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/ \n",
    "    #->+ https://www.kdnuggets.com/2018/02/simple-starter-guide-build-neural-network.html\n",
    "    #->+ https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py\n",
    "    #->+ Highly detailed and some useful code snippets - https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
    "#-> Titanic example (using Softmax) - https://www.kaggle.com/tauseef6462/simple-feedforward-neural-network-using-pytorch \n",
    "#-> other shorter examples - https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb\n",
    "    #->+ https://medium.com/@niranjankumarc/building-a-feedforward-neural-network-using-pytorch-nn-module-52b1d7ea5c3e\n",
    "#-> dataloaders - https://towardsdatascience.com/pytorch-basics-intro-to-dataloaders-and-loss-functions-868e86450047 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4683, 31) [[ 1.79822108e+00  1.38564230e+00 -1.26787020e-01 -2.64358581e-01\n",
      "  -6.64585064e-01 -3.18171165e-01  2.97652982e+00 -9.44792401e-01\n",
      "   2.07457949e+00 -1.55962957e-02  1.95613404e+00 -7.33284252e-01\n",
      "  -2.55165589e-01 -2.40230399e-01  2.36572896e+00 -2.52146777e+00\n",
      "   1.27402325e-01 -3.10296251e-02  1.29081691e-01  2.16085459e-01\n",
      "  -4.65851479e-01 -9.51843362e-01  8.36060371e-01  5.82307505e-01\n",
      "   4.21409810e-01 -1.03425498e+00 -2.08494454e+00  2.09337045e+00\n",
      "  -7.45183857e-01  9.43303409e-01 -4.16962328e-01]\n",
      " [-1.19283157e-01 -8.66875549e-01 -1.26787020e-01 -4.39616866e-01\n",
      "   6.87377628e-02 -1.24419598e+00  1.55258980e+00 -3.12740071e-03\n",
      "  -4.75072688e-01 -1.22155483e+00 -1.27723900e-02  1.47911251e+00\n",
      "   1.07026093e-01 -3.31017085e-01  5.16887532e-01  1.81145068e-01\n",
      "  -8.52389837e-01  5.80389740e-01  5.91351783e-01  6.54761946e-01\n",
      "   1.52169956e+00  1.21552152e+00  9.06974619e-01  8.17190512e-01\n",
      "  -3.38466638e+00  6.55822101e-01 -1.71565351e+00 -1.98551263e+00\n",
      "  -3.60881449e-02 -1.05320865e+00  1.51097695e-01]\n",
      " [ 1.17333653e+00  3.52151783e-01 -1.26787020e-01  1.02969677e-01\n",
      "   7.80582282e-01 -3.18171165e-01  2.72605486e-01 -4.24739361e-01\n",
      "   1.19489060e-01  1.07694322e-03  1.18670598e-01  1.89081687e-01\n",
      "  -5.00712229e-01  7.39745813e-01 -3.53674887e-01  8.38656704e-03\n",
      "  -1.18396452e+00  1.72975481e+00 -2.94022035e-02 -1.92462997e+00\n",
      "   3.74813149e-01 -2.90185363e-01 -1.21773814e+00 -9.67801732e-02\n",
      "   8.43965498e-01 -9.10086048e-01  1.55726213e-01  7.47834899e-01\n",
      "  -1.38063000e+00  9.43303409e-01 -5.73757414e-01]\n",
      " [-8.49302383e-01  4.75069120e-01  6.18607513e-01 -2.70433594e+00\n",
      "   8.68218298e-02 -3.18171165e-01  2.52604143e+00  4.92032493e-01\n",
      "   1.35994484e+00 -5.04171075e-01  1.28818770e-01  6.50519967e-02\n",
      "   2.62657976e+00 -7.89832089e-01 -1.19272404e+00  5.13088188e-01\n",
      "  -8.73586639e-01 -3.59533617e-01  5.87792426e-01  8.21439908e-01\n",
      "  -9.86632870e-01 -7.94519534e-01 -8.08928582e-01  6.73620350e-02\n",
      "  -4.26264064e-01 -3.97916594e-01 -8.62113100e-01  4.41635462e-02\n",
      "   2.96015054e-01 -4.23539464e-01  6.29951254e-01]\n",
      " [ 4.79087099e-01  7.37909359e-02 -8.43590544e-01 -8.39897559e-01\n",
      "  -2.14457814e-01  1.74861186e+00 -6.10320780e-01 -1.33126820e+00\n",
      "  -6.33118534e-02 -1.03694657e-01 -5.82797379e-01 -2.08314542e+00\n",
      "  -9.53566971e-02  1.08244313e+00 -8.03911615e-01 -2.37488441e+00\n",
      "   2.18152857e+00  1.28751526e+00  2.08138504e+00  7.57020785e-01\n",
      "   1.42317442e+00 -1.27433129e+00 -2.28187465e+00  5.27519842e-03\n",
      "   8.17301947e-01  1.71298119e-01 -1.24344537e-01 -8.36755082e-01\n",
      "  -8.59781810e-01 -1.24761382e+00  3.07284425e-02]]\n",
      "(4683,) [1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#get data into train and test splits and convert to numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(prs.iloc[:, 3:len(prs)].to_numpy(), prs[\"Case\"].to_numpy(), test_size=0.25, random_state=0)\n",
    "\n",
    "print(x_train.shape, x_train[0:5])\n",
    "print(y_train.shape, y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to torch tensors\n",
    "x_train, y_train, x_test, y_test = map(torch.tensor, (x_train, y_train, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4683, 31]) tensor([[ 1.7982e+00,  1.3856e+00, -1.2679e-01, -2.6436e-01, -6.6459e-01,\n",
      "         -3.1817e-01,  2.9765e+00, -9.4479e-01,  2.0746e+00, -1.5596e-02,\n",
      "          1.9561e+00, -7.3328e-01, -2.5517e-01, -2.4023e-01,  2.3657e+00,\n",
      "         -2.5215e+00,  1.2740e-01, -3.1030e-02,  1.2908e-01,  2.1609e-01,\n",
      "         -4.6585e-01, -9.5184e-01,  8.3606e-01,  5.8231e-01,  4.2141e-01,\n",
      "         -1.0343e+00, -2.0849e+00,  2.0934e+00, -7.4518e-01,  9.4330e-01,\n",
      "         -4.1696e-01],\n",
      "        [-1.1928e-01, -8.6688e-01, -1.2679e-01, -4.3962e-01,  6.8738e-02,\n",
      "         -1.2442e+00,  1.5526e+00, -3.1274e-03, -4.7507e-01, -1.2216e+00,\n",
      "         -1.2772e-02,  1.4791e+00,  1.0703e-01, -3.3102e-01,  5.1689e-01,\n",
      "          1.8115e-01, -8.5239e-01,  5.8039e-01,  5.9135e-01,  6.5476e-01,\n",
      "          1.5217e+00,  1.2155e+00,  9.0697e-01,  8.1719e-01, -3.3847e+00,\n",
      "          6.5582e-01, -1.7157e+00, -1.9855e+00, -3.6088e-02, -1.0532e+00,\n",
      "          1.5110e-01],\n",
      "        [ 1.1733e+00,  3.5215e-01, -1.2679e-01,  1.0297e-01,  7.8058e-01,\n",
      "         -3.1817e-01,  2.7261e-01, -4.2474e-01,  1.1949e-01,  1.0769e-03,\n",
      "          1.1867e-01,  1.8908e-01, -5.0071e-01,  7.3975e-01, -3.5367e-01,\n",
      "          8.3866e-03, -1.1840e+00,  1.7298e+00, -2.9402e-02, -1.9246e+00,\n",
      "          3.7481e-01, -2.9019e-01, -1.2177e+00, -9.6780e-02,  8.4397e-01,\n",
      "         -9.1009e-01,  1.5573e-01,  7.4783e-01, -1.3806e+00,  9.4330e-01,\n",
      "         -5.7376e-01],\n",
      "        [-8.4930e-01,  4.7507e-01,  6.1861e-01, -2.7043e+00,  8.6822e-02,\n",
      "         -3.1817e-01,  2.5260e+00,  4.9203e-01,  1.3599e+00, -5.0417e-01,\n",
      "          1.2882e-01,  6.5052e-02,  2.6266e+00, -7.8983e-01, -1.1927e+00,\n",
      "          5.1309e-01, -8.7359e-01, -3.5953e-01,  5.8779e-01,  8.2144e-01,\n",
      "         -9.8663e-01, -7.9452e-01, -8.0893e-01,  6.7362e-02, -4.2626e-01,\n",
      "         -3.9792e-01, -8.6211e-01,  4.4164e-02,  2.9602e-01, -4.2354e-01,\n",
      "          6.2995e-01],\n",
      "        [ 4.7909e-01,  7.3791e-02, -8.4359e-01, -8.3990e-01, -2.1446e-01,\n",
      "          1.7486e+00, -6.1032e-01, -1.3313e+00, -6.3312e-02, -1.0369e-01,\n",
      "         -5.8280e-01, -2.0831e+00, -9.5357e-02,  1.0824e+00, -8.0391e-01,\n",
      "         -2.3749e+00,  2.1815e+00,  1.2875e+00,  2.0814e+00,  7.5702e-01,\n",
      "          1.4232e+00, -1.2743e+00, -2.2819e+00,  5.2752e-03,  8.1730e-01,\n",
      "          1.7130e-01, -1.2434e-01, -8.3676e-01, -8.5978e-01, -1.2476e+00,\n",
      "          3.0728e-02]], dtype=torch.float64)\n",
      "torch.Size([4683]) tensor([1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.size(), x_train[0:5])\n",
    "print(y_train.size(), y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup torch datasets to load into dataloaders\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "training_data = CustomDataset(x_train, y_train)\n",
    "test_data = CustomDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup dataloaders to handle batches\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(x_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 1-hidden layer neural net (31 -> 50 -> 1) with a SGD optimiser and a BCE Loss objective function\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #initializing layers as per Microsoft article (but does not seem to be a requirement)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(31, 50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.out = nn.Linear(50, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight) \n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight) \n",
    "        torch.nn.init.zeros_(self.out.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        a1 = self.fc1(input)\n",
    "        h1 = self.relu1(a1)\n",
    "        a2 = self.out(h1)\n",
    "        y = self.out_act(a2)\n",
    "        return y\n",
    "    \n",
    "net = Net()\n",
    "opt = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 32.719083070755005\n",
      "Epoch: 2 | Loss: 31.179018139839172\n",
      "Epoch: 3 | Loss: 30.470041632652283\n",
      "Epoch: 4 | Loss: 29.989330172538757\n",
      "Epoch: 5 | Loss: 29.652220726013184\n",
      "Epoch: 6 | Loss: 29.362329065799713\n",
      "Epoch: 7 | Loss: 29.154961466789246\n",
      "Epoch: 8 | Loss: 28.94689705967903\n",
      "Epoch: 9 | Loss: 28.805283427238464\n",
      "Epoch: 10 | Loss: 28.654724836349487\n",
      "Epoch: 11 | Loss: 28.54733121395111\n",
      "Epoch: 12 | Loss: 28.458424150943756\n",
      "Epoch: 13 | Loss: 28.364082098007202\n",
      "Epoch: 14 | Loss: 28.287536025047302\n",
      "Epoch: 15 | Loss: 28.21243464946747\n",
      "Epoch: 16 | Loss: 28.157539129257202\n",
      "Epoch: 17 | Loss: 28.105873227119446\n",
      "Epoch: 18 | Loss: 28.06497198343277\n",
      "Epoch: 19 | Loss: 28.00512307882309\n",
      "Epoch: 20 | Loss: 27.96257209777832\n",
      "Epoch: 21 | Loss: 27.915198743343353\n",
      "Epoch: 22 | Loss: 27.87716841697693\n",
      "Epoch: 23 | Loss: 27.854857087135315\n",
      "Epoch: 24 | Loss: 27.81659686565399\n",
      "Epoch: 25 | Loss: 27.789813995361328\n",
      "Epoch: 26 | Loss: 27.750869691371918\n",
      "Epoch: 27 | Loss: 27.723398327827454\n",
      "Epoch: 28 | Loss: 27.697824895381927\n",
      "Epoch: 29 | Loss: 27.676651775836945\n",
      "Epoch: 30 | Loss: 27.642711400985718\n",
      "Epoch: 31 | Loss: 27.630333125591278\n",
      "Epoch: 32 | Loss: 27.60260307788849\n",
      "Epoch: 33 | Loss: 27.594671428203583\n",
      "Epoch: 34 | Loss: 27.568907141685486\n",
      "Epoch: 35 | Loss: 27.530251145362854\n",
      "Epoch: 36 | Loss: 27.525572299957275\n",
      "Epoch: 37 | Loss: 27.496862918138504\n",
      "Epoch: 38 | Loss: 27.494583517313004\n",
      "Epoch: 39 | Loss: 27.462768644094467\n",
      "Epoch: 40 | Loss: 27.439529985189438\n",
      "Epoch: 41 | Loss: 27.4207022190094\n",
      "Epoch: 42 | Loss: 27.402380645275116\n",
      "Epoch: 43 | Loss: 27.4009929895401\n",
      "Epoch: 44 | Loss: 27.366080045700073\n",
      "Epoch: 45 | Loss: 27.36682227253914\n",
      "Epoch: 46 | Loss: 27.353010416030884\n",
      "Epoch: 47 | Loss: 27.335506200790405\n",
      "Epoch: 48 | Loss: 27.3110910654068\n",
      "Epoch: 49 | Loss: 27.281039118766785\n",
      "Epoch: 50 | Loss: 27.26956957578659\n",
      "Epoch: 51 | Loss: 27.26442265510559\n",
      "Epoch: 52 | Loss: 27.25448715686798\n",
      "Epoch: 53 | Loss: 27.245286762714386\n",
      "Epoch: 54 | Loss: 27.206596851348877\n",
      "Epoch: 55 | Loss: 27.201833963394165\n",
      "Epoch: 56 | Loss: 27.202169001102448\n",
      "Epoch: 57 | Loss: 27.17515540122986\n",
      "Epoch: 58 | Loss: 27.16541188955307\n",
      "Epoch: 59 | Loss: 27.149583727121353\n",
      "Epoch: 60 | Loss: 27.137609243392944\n",
      "Epoch: 61 | Loss: 27.123367130756378\n",
      "Epoch: 62 | Loss: 27.12367355823517\n",
      "Epoch: 63 | Loss: 27.10579577088356\n",
      "Epoch: 64 | Loss: 27.09212702512741\n",
      "Epoch: 65 | Loss: 27.07462853193283\n",
      "Epoch: 66 | Loss: 27.073021113872528\n",
      "Epoch: 67 | Loss: 27.062323719263077\n",
      "Epoch: 68 | Loss: 27.0528604388237\n",
      "Epoch: 69 | Loss: 27.02828025817871\n",
      "Epoch: 70 | Loss: 27.01967579126358\n",
      "Epoch: 71 | Loss: 27.010723888874054\n",
      "Epoch: 72 | Loss: 26.999117970466614\n",
      "Epoch: 73 | Loss: 26.99594807624817\n",
      "Epoch: 74 | Loss: 26.97640472650528\n",
      "Epoch: 75 | Loss: 26.951607823371887\n",
      "Epoch: 76 | Loss: 26.956532508134842\n",
      "Epoch: 77 | Loss: 26.94527843594551\n",
      "Epoch: 78 | Loss: 26.93463695049286\n",
      "Epoch: 79 | Loss: 26.929783314466476\n",
      "Epoch: 80 | Loss: 26.91446563601494\n",
      "Epoch: 81 | Loss: 26.899072587490082\n",
      "Epoch: 82 | Loss: 26.88939279317856\n",
      "Epoch: 83 | Loss: 26.874874770641327\n",
      "Epoch: 84 | Loss: 26.871621131896973\n",
      "Epoch: 85 | Loss: 26.85916617512703\n",
      "Epoch: 86 | Loss: 26.87257432937622\n",
      "Epoch: 87 | Loss: 26.84343993663788\n",
      "Epoch: 88 | Loss: 26.82961654663086\n",
      "Epoch: 89 | Loss: 26.843602061271667\n",
      "Epoch: 90 | Loss: 26.807125508785248\n",
      "Epoch: 91 | Loss: 26.80265697836876\n",
      "Epoch: 92 | Loss: 26.80230662226677\n",
      "Epoch: 93 | Loss: 26.794390588998795\n",
      "Epoch: 94 | Loss: 26.797093868255615\n",
      "Epoch: 95 | Loss: 26.75708857178688\n",
      "Epoch: 96 | Loss: 26.760373443365097\n",
      "Epoch: 97 | Loss: 26.753740668296814\n",
      "Epoch: 98 | Loss: 26.739012241363525\n",
      "Epoch: 99 | Loss: 26.73789569735527\n",
      "Epoch: 100 | Loss: 26.728598475456238\n",
      "Epoch: 101 | Loss: 26.705748438835144\n",
      "Epoch: 102 | Loss: 26.696863800287247\n",
      "Epoch: 103 | Loss: 26.699361354112625\n",
      "Epoch: 104 | Loss: 26.69435253739357\n",
      "Epoch: 105 | Loss: 26.684450179338455\n",
      "Epoch: 106 | Loss: 26.6768416762352\n",
      "Epoch: 107 | Loss: 26.66846537590027\n",
      "Epoch: 108 | Loss: 26.659090250730515\n",
      "Epoch: 109 | Loss: 26.628356903791428\n",
      "Epoch: 110 | Loss: 26.637354254722595\n",
      "Epoch: 111 | Loss: 26.643403977155685\n",
      "Epoch: 112 | Loss: 26.629934787750244\n",
      "Epoch: 113 | Loss: 26.617305755615234\n",
      "Epoch: 114 | Loss: 26.606850564479828\n",
      "Epoch: 115 | Loss: 26.58970245718956\n",
      "Epoch: 116 | Loss: 26.581472486257553\n",
      "Epoch: 117 | Loss: 26.567566722631454\n",
      "Epoch: 118 | Loss: 26.576461166143417\n",
      "Epoch: 119 | Loss: 26.55329030752182\n",
      "Epoch: 120 | Loss: 26.54887717962265\n",
      "Epoch: 121 | Loss: 26.53634923696518\n",
      "Epoch: 122 | Loss: 26.538503021001816\n",
      "Epoch: 123 | Loss: 26.52740827202797\n",
      "Epoch: 124 | Loss: 26.517954647541046\n",
      "Epoch: 125 | Loss: 26.51567730307579\n",
      "Epoch: 126 | Loss: 26.50072744488716\n",
      "Epoch: 127 | Loss: 26.5031720995903\n",
      "Epoch: 128 | Loss: 26.485609382390976\n",
      "Epoch: 129 | Loss: 26.484490036964417\n",
      "Epoch: 130 | Loss: 26.470856994390488\n",
      "Epoch: 131 | Loss: 26.456485360860825\n",
      "Epoch: 132 | Loss: 26.448146045207977\n",
      "Epoch: 133 | Loss: 26.44401988387108\n",
      "Epoch: 134 | Loss: 26.43991595506668\n",
      "Epoch: 135 | Loss: 26.442689806222916\n",
      "Epoch: 136 | Loss: 26.420289427042007\n",
      "Epoch: 137 | Loss: 26.421547532081604\n",
      "Epoch: 138 | Loss: 26.406684696674347\n",
      "Epoch: 139 | Loss: 26.409766763448715\n",
      "Epoch: 140 | Loss: 26.392675429582596\n",
      "Epoch: 141 | Loss: 26.37403354048729\n",
      "Epoch: 142 | Loss: 26.371054619550705\n",
      "Epoch: 143 | Loss: 26.365811198949814\n",
      "Epoch: 144 | Loss: 26.348089933395386\n",
      "Epoch: 145 | Loss: 26.34664613008499\n",
      "Epoch: 146 | Loss: 26.33324959874153\n",
      "Epoch: 147 | Loss: 26.33756360411644\n",
      "Epoch: 148 | Loss: 26.329553306102753\n",
      "Epoch: 149 | Loss: 26.308290660381317\n",
      "Epoch: 150 | Loss: 26.308729022741318\n",
      "Epoch: 151 | Loss: 26.296446949243546\n",
      "Epoch: 152 | Loss: 26.289807051420212\n",
      "Epoch: 153 | Loss: 26.30164584517479\n",
      "Epoch: 154 | Loss: 26.270440757274628\n",
      "Epoch: 155 | Loss: 26.26071935892105\n",
      "Epoch: 156 | Loss: 26.26663511991501\n",
      "Epoch: 157 | Loss: 26.251950711011887\n",
      "Epoch: 158 | Loss: 26.250348031520844\n",
      "Epoch: 159 | Loss: 26.235071182250977\n",
      "Epoch: 160 | Loss: 26.222186118364334\n",
      "Epoch: 161 | Loss: 26.222112208604813\n",
      "Epoch: 162 | Loss: 26.21841451525688\n",
      "Epoch: 163 | Loss: 26.214620977640152\n",
      "Epoch: 164 | Loss: 26.196615666151047\n",
      "Epoch: 165 | Loss: 26.18934652209282\n",
      "Epoch: 166 | Loss: 26.174180418252945\n",
      "Epoch: 167 | Loss: 26.182884603738785\n",
      "Epoch: 168 | Loss: 26.168381363153458\n",
      "Epoch: 169 | Loss: 26.152612328529358\n",
      "Epoch: 170 | Loss: 26.15182289481163\n",
      "Epoch: 171 | Loss: 26.15663355588913\n",
      "Epoch: 172 | Loss: 26.139184653759003\n",
      "Epoch: 173 | Loss: 26.131167948246002\n",
      "Epoch: 174 | Loss: 26.123105883598328\n",
      "Epoch: 175 | Loss: 26.12410879135132\n",
      "Epoch: 176 | Loss: 26.116204977035522\n",
      "Epoch: 177 | Loss: 26.095336616039276\n",
      "Epoch: 178 | Loss: 26.097819983959198\n",
      "Epoch: 179 | Loss: 26.07992920279503\n",
      "Epoch: 180 | Loss: 26.0753056704998\n",
      "Epoch: 181 | Loss: 26.06447207927704\n",
      "Epoch: 182 | Loss: 26.049886256456375\n",
      "Epoch: 183 | Loss: 26.052216410636902\n",
      "Epoch: 184 | Loss: 26.036371648311615\n",
      "Epoch: 185 | Loss: 26.037097543478012\n",
      "Epoch: 186 | Loss: 26.030108779668808\n",
      "Epoch: 187 | Loss: 26.025323510169983\n",
      "Epoch: 188 | Loss: 26.017740964889526\n",
      "Epoch: 189 | Loss: 25.995411336421967\n",
      "Epoch: 190 | Loss: 25.997983187437057\n",
      "Epoch: 191 | Loss: 25.980128705501556\n",
      "Epoch: 192 | Loss: 25.971992015838623\n",
      "Epoch: 193 | Loss: 25.957603126764297\n",
      "Epoch: 194 | Loss: 25.964061558246613\n",
      "Epoch: 195 | Loss: 25.957002103328705\n",
      "Epoch: 196 | Loss: 25.952535212039948\n",
      "Epoch: 197 | Loss: 25.949621945619583\n",
      "Epoch: 198 | Loss: 25.93719306588173\n",
      "Epoch: 199 | Loss: 25.9272438287735\n",
      "Epoch: 200 | Loss: 25.90650451183319\n",
      "Epoch: 201 | Loss: 25.90930810570717\n",
      "Epoch: 202 | Loss: 25.893731832504272\n",
      "Epoch: 203 | Loss: 25.88108804821968\n",
      "Epoch: 204 | Loss: 25.888779044151306\n",
      "Epoch: 205 | Loss: 25.86102330684662\n",
      "Epoch: 206 | Loss: 25.877282559871674\n",
      "Epoch: 207 | Loss: 25.868913799524307\n",
      "Epoch: 208 | Loss: 25.856445848941803\n",
      "Epoch: 209 | Loss: 25.84487807750702\n",
      "Epoch: 210 | Loss: 25.83200290799141\n",
      "Epoch: 211 | Loss: 25.831105768680573\n",
      "Epoch: 212 | Loss: 25.80805829167366\n",
      "Epoch: 213 | Loss: 25.82118707895279\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "#turn training on \n",
    "net.train()\n",
    "\n",
    "#loop through epochs\n",
    "for e in range(1,num_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    #loop through batches\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        X_batch = batch[0].float()\n",
    "        y_batch = batch[1].float()\n",
    "        #zero the gradient\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        #predict the output\n",
    "        y_batch_pred = net(X_batch)\n",
    "        \n",
    "        #calculate the loss (check expected function dimensions)\n",
    "        loss = criterion(y_batch_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #update the weights\n",
    "        opt.step()\n",
    "        \n",
    "        #accumulate epoch losses\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    #present epoch outputs\n",
    "    print(\"Epoch: \" + str(e) + \" | \" + \"Loss: \" + str(epoch_loss))\n",
    "\n",
    "#Next steps \n",
    "    #-> build evaluation to test accuracy of predictions\n",
    "    #-> review if setup correct / how to add improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5751, 0.5911, 0.7144, 0.6459, 0.6608, 0.6427, 0.5824, 0.9222, 0.6920,\n",
      "        0.5816, 0.7630, 0.3597, 0.7692, 0.8987, 0.4243, 0.6986, 0.4913, 0.6223,\n",
      "        0.7749, 0.4378, 0.7197, 0.5926, 0.5797, 0.3618, 0.6172, 0.9625, 0.7680,\n",
      "        0.6439, 0.5291, 0.3537, 0.2973, 0.8067, 0.5505, 0.7727, 0.6406, 0.6414,\n",
      "        0.6594, 0.2884, 0.6111, 0.3640, 0.9343, 0.6750, 0.6052, 0.5951, 0.6089,\n",
      "        0.6143, 0.8803, 0.8883, 0.8484, 0.3554, 0.8485, 0.5950, 0.9541, 0.6667,\n",
      "        0.6288, 0.4770, 0.7058, 0.7369, 0.2867, 0.8017, 0.5314, 0.5680, 0.9002,\n",
      "        0.4736, 0.6045, 0.8249, 0.6063, 0.8039, 0.5714, 0.7870, 0.8556, 0.6451,\n",
      "        0.4531, 0.9079, 0.5328, 0.6302, 0.6715, 0.6787, 0.7251, 0.7702, 0.9409,\n",
      "        0.6854, 0.5490], grad_fn=<ViewBackward>)\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(y_batch_pred.view(-1))\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVIEW IF REQUIRED\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "def assess_model_performance(md, pred, X_test, y_test, kernel, plot=True):\n",
    "    accuracy_sk = md.score(X_test, y_test)\n",
    "    auc = roc_auc_score(y_test, md.predict(X_test))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    sensitivity = tp / (tp + fn) #-> also recall\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    result = pd.DataFrame({\"kernel\":kernel, \n",
    "                           \"accuracy\":accuracy_sk, \n",
    "                           \"sensitivity\": sensitivity, \n",
    "                           \"specificity\":specificity,\n",
    "                           \"precision\":precision,\n",
    "                           \"auc\":auc}, index=[1])\n",
    "    \n",
    "    if plot:\n",
    "        probas = md.predict_proba(X_test)\n",
    "        plt.plot(roc_curve(y_test, probas[:,1])[0], roc_curve(y_test, probas[:,1])[1], label=kernel)\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.legend(prop={'size':10}, loc='lower right')\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
